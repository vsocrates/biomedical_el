Restoring modules from user's fosscuda111, for system: "farnam-rhel7"
/home/vs428/project/MedMentions/full/pretraining5/entity_vocab.jsonl
Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
0
<torch.cuda.device object at 0x2af0f2f8eb20>
1
GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "train.py", line 117, in <module>
    train(model, train_dataloader, optimizer, mention_entity_loss, epoch, pretrained_entity_embeddings, device)
  File "/gpfs/ysm/home/vs428/Documents/entity_linking/biomedical_el/entitylinker/train_utils.py", line 255, in train
    mention_pred, entity_pred = model(input_ids,token_type_ids,attention_mask)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/home/vs428/Documents/entity_linking/biomedical_el/entitylinker/model.py", line 117, in forward
    outputs = self.pretrained_model(word_ids, token_type_ids, attention_mask)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 971, in forward
    encoder_outputs = self.encoder(
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 568, in forward
    layer_outputs = layer_module(
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 456, in forward
    self_attention_outputs = self.attention(
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 387, in forward
    self_outputs = self.self(
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 319, in forward
    attention_probs = self.dropout(attention_probs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/dropout.py", line 58, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/functional.py", line 1076, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
RuntimeError: CUDA out of memory. Tried to allocate 132.00 MiB (GPU 0; 10.92 GiB total capacity; 10.07 GiB already allocated; 59.44 MiB free; 10.13 GiB reserved in total by PyTorch)
