Restoring modules from user's fosscuda111, for system: "farnam-rhel7"
/home/vs428/project/MedMentions/full/pretraining5/entity_vocab.jsonl
Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
0
<torch.cuda.device object at 0x2b1474a08b20>
1
GeForce GTX 1080 Ti
Traceback (most recent call last):
  File "train.py", line 117, in <module>
    train(model, train_dataloader, optimizer, mention_entity_loss, epoch, pretrained_entity_embeddings, device)
  File "/gpfs/ysm/home/vs428/Documents/entity_linking/biomedical_el/entitylinker/train_utils.py", line 255, in train
    mention_pred, entity_pred = model(input_ids,token_type_ids,attention_mask)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/home/vs428/Documents/entity_linking/biomedical_el/entitylinker/model.py", line 117, in forward
    outputs = self.pretrained_model(word_ids, token_type_ids, attention_mask)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 971, in forward
    encoder_outputs = self.encoder(
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 568, in forward
    layer_outputs = layer_module(
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 456, in forward
    self_attention_outputs = self.attention(
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 387, in forward
    self_outputs = self.self(
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/torch/nn/modules/module.py", line 889, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/gpfs/ysm/project/rtaylor/vs428/conda_envs/biomed_el111/lib/python3.8/site-packages/transformers/models/bert/modeling_bert.py", line 291, in forward
    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))
RuntimeError: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 10.92 GiB total capacity; 9.97 GiB already allocated; 25.44 MiB free; 10.16 GiB reserved in total by PyTorch)
