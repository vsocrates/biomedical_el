Restoring modules from user's fosscuda111, for system: "farnam-rhel7"
/home/vs428/project/MedMentions/full/pretraining5/entity_vocab.jsonl
Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
0
<torch.cuda.device object at 0x2b97447c4b20>
1
GeForce GTX 1080 Ti
[1,     5] train loss: 1.008
[1,    10] train loss: 1.010
[1,    15] train loss: 1.008
[1,    20] train loss: 1.006
[1,    25] train loss: 1.002
[1,    30] train loss: 0.999
[1,    35] train loss: 0.999
[1,    40] train loss: 0.999
[1,    45] train loss: 1.000
[1,    50] train loss: 0.993
[1,    55] train loss: 0.997
[1,    60] train loss: 0.996
[1,    65] train loss: 0.990
[1,    70] train loss: 0.992
[1,    75] train loss: 0.990
[1,    80] train loss: 0.991
[1,    85] train loss: 0.987
[1,    90] train loss: 0.987
[1,    95] train loss: 0.989
[1,   100] train loss: 0.983
[1,   105] train loss: 0.987
[1,   110] train loss: 0.989
[1,   115] train loss: 0.983
[1,   120] train loss: 0.983
[1,   125] train loss: 0.981
[1,   130] train loss: 0.984
[1,   135] train loss: 0.981
[1,   140] train loss: 0.977
[1,   145] train loss: 0.980
[1,   150] train loss: 0.978
[1,   155] train loss: 0.972
[1,   160] train loss: 0.979
[1,   165] train loss: 0.966
[1,   170] train loss: 0.968
[1,   175] train loss: 0.962
[1,   180] train loss: 0.966
[1,   185] train loss: 0.966
[1,   190] train loss: 0.960
[1,   195] train loss: 0.960
[1,   200] train loss: 0.953
[1,   205] train loss: 0.957
[1,   210] train loss: 0.958
[1,   215] train loss: 0.950
[1,   220] train loss: 0.954
[1,   225] train loss: 0.950
[1,   230] train loss: 0.952
[1,   235] train loss: 0.946
[1,   240] train loss: 0.949
[1,   245] train loss: 0.943
[1,   250] train loss: 0.944
[1,   255] train loss: 0.952
[1,   260] train loss: 0.944
[1,   265] train loss: 0.945
[1,   270] train loss: 0.943
[1,   275] train loss: 0.940
[1,   280] train loss: 0.943
[1,   285] train loss: 0.943
[1,   290] train loss: 0.938
[1,   295] train loss: 0.935
[1,   300] train loss: 0.934
[1,   305] train loss: 0.937
[1,   310] train loss: 0.930
[1,   315] train loss: 0.931
[1,   320] train loss: 0.932
[1,   325] train loss: 0.936
[1,   330] train loss: 0.933
Finished Training
[1,     5] test loss: 0.931
[1,    10] test loss: 0.931
[1,    15] test loss: 0.928
[1,    20] test loss: 0.929
[1,    25] test loss: 0.919
[1,    30] test loss: 0.930
[1,    35] test loss: 0.931
[1,    40] test loss: 0.924
[1,    45] test loss: 0.929
[1,    50] test loss: 0.922
[1,    55] test loss: 0.926
[1,    60] test loss: 0.932
[1,    65] test loss: 0.932
[1,    70] test loss: 0.924
[1,    75] test loss: 0.931
[1,    80] test loss: 0.928
[1,    85] test loss: 0.927
[1,    90] test loss: 0.925
[1,    95] test loss: 0.924
[1,   100] test loss: 0.927
[1,   105] test loss: 0.929
[1,   110] test loss: 0.923
-----------------------------------------------------------------------------------------
Micro-Precision: 0.00838499702513218, Macro-Precision: nan

Micro-Recall: 0.013081459328532219, Macro-Recall: nan

Micro-F1: 0.010219478979706764, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 126.43s | valid loss  0.94 | valid ppl     2.55
-----------------------------------------------------------------------------------------
[2,     5] train loss: 0.932
[2,    10] train loss: 0.926
[2,    15] train loss: 0.930
[2,    20] train loss: 0.932
[2,    25] train loss: 0.925
[2,    30] train loss: 0.929
[2,    35] train loss: 0.931
[2,    40] train loss: 0.926
[2,    45] train loss: 0.924
[2,    50] train loss: 0.916
[2,    55] train loss: 0.926
[2,    60] train loss: 0.923
[2,    65] train loss: 0.929
[2,    70] train loss: 0.925
[2,    75] train loss: 0.923
[2,    80] train loss: 0.924
[2,    85] train loss: 0.918
[2,    90] train loss: 0.924
[2,    95] train loss: 0.920
[2,   100] train loss: 0.918
[2,   105] train loss: 0.922
[2,   110] train loss: 0.917
[2,   115] train loss: 0.914
[2,   120] train loss: 0.912
[2,   125] train loss: 0.921
[2,   130] train loss: 0.917
[2,   135] train loss: 0.910
[2,   140] train loss: 0.916
[2,   145] train loss: 0.925
[2,   150] train loss: 0.912
[2,   155] train loss: 0.909
[2,   160] train loss: 0.911
[2,   165] train loss: 0.902
[2,   170] train loss: 0.908
[2,   175] train loss: 0.909
[2,   180] train loss: 0.910
[2,   185] train loss: 0.905
[2,   190] train loss: 0.903
[2,   195] train loss: 0.906
[2,   200] train loss: 0.906
[2,   205] train loss: 0.901
[2,   210] train loss: 0.913
[2,   215] train loss: 0.906
[2,   220] train loss: 0.901
[2,   225] train loss: 0.901
[2,   230] train loss: 0.901
[2,   235] train loss: 0.903
[2,   240] train loss: 0.901
[2,   245] train loss: 0.898
[2,   250] train loss: 0.894
[2,   255] train loss: 0.897
[2,   260] train loss: 0.899
[2,   265] train loss: 0.899
[2,   270] train loss: 0.903
[2,   275] train loss: 0.890
[2,   280] train loss: 0.898
[2,   285] train loss: 0.896
[2,   290] train loss: 0.890
[2,   295] train loss: 0.881
[2,   300] train loss: 0.898
[2,   305] train loss: 0.890
[2,   310] train loss: 0.894
[2,   315] train loss: 0.889
[2,   320] train loss: 0.897
[2,   325] train loss: 0.891
[2,   330] train loss: 0.888
Finished Training
[2,     5] test loss: 0.875
[2,    10] test loss: 0.879
[2,    15] test loss: 0.872
[2,    20] test loss: 0.885
[2,    25] test loss: 0.879
[2,    30] test loss: 0.888
[2,    35] test loss: 0.878
[2,    40] test loss: 0.870
[2,    45] test loss: 0.878
[2,    50] test loss: 0.876
[2,    55] test loss: 0.887
[2,    60] test loss: 0.883
[2,    65] test loss: 0.877
[2,    70] test loss: 0.884
[2,    75] test loss: 0.886
[2,    80] test loss: 0.881
[2,    85] test loss: 0.881
[2,    90] test loss: 0.886
[2,    95] test loss: 0.878
[2,   100] test loss: 0.872
[2,   105] test loss: 0.882
[2,   110] test loss: 0.876
-----------------------------------------------------------------------------------------
Micro-Precision: 0.04419540986418724, Macro-Precision: nan

Micro-Recall: 0.04818624630570412, Macro-Recall: nan

Micro-F1: 0.04610462859272957, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 133.07s | valid loss  0.89 | valid ppl     2.43
-----------------------------------------------------------------------------------------
[3,     5] train loss: 0.884
[3,    10] train loss: 0.887
[3,    15] train loss: 0.883
[3,    20] train loss: 0.893
[3,    25] train loss: 0.881
[3,    30] train loss: 0.881
[3,    35] train loss: 0.875
[3,    40] train loss: 0.884
[3,    45] train loss: 0.877
[3,    50] train loss: 0.882
[3,    55] train loss: 0.887
[3,    60] train loss: 0.880
[3,    65] train loss: 0.880
[3,    70] train loss: 0.888
[3,    75] train loss: 0.885
[3,    80] train loss: 0.876
[3,    85] train loss: 0.886
[3,    90] train loss: 0.888
[3,    95] train loss: 0.878
[3,   100] train loss: 0.886
[3,   105] train loss: 0.866
[3,   110] train loss: 0.880
[3,   115] train loss: 0.874
[3,   120] train loss: 0.877
[3,   125] train loss: 0.871
[3,   130] train loss: 0.875
[3,   135] train loss: 0.868
[3,   140] train loss: 0.876
[3,   145] train loss: 0.882
[3,   150] train loss: 0.877
[3,   155] train loss: 0.877
[3,   160] train loss: 0.875
[3,   165] train loss: 0.880
[3,   170] train loss: 0.862
[3,   175] train loss: 0.877
[3,   180] train loss: 0.881
[3,   185] train loss: 0.871
[3,   190] train loss: 0.876
[3,   195] train loss: 0.871
[3,   200] train loss: 0.871
[3,   205] train loss: 0.879
[3,   210] train loss: 0.865
[3,   215] train loss: 0.874
[3,   220] train loss: 0.870
[3,   225] train loss: 0.877
[3,   230] train loss: 0.870
[3,   235] train loss: 0.863
[3,   240] train loss: 0.869
[3,   245] train loss: 0.878
[3,   250] train loss: 0.872
[3,   255] train loss: 0.856
[3,   260] train loss: 0.874
[3,   265] train loss: 0.873
[3,   270] train loss: 0.867
[3,   275] train loss: 0.871
[3,   280] train loss: 0.873
[3,   285] train loss: 0.867
[3,   290] train loss: 0.862
[3,   295] train loss: 0.856
[3,   300] train loss: 0.866
[3,   305] train loss: 0.862
[3,   310] train loss: 0.869
[3,   315] train loss: 0.857
[3,   320] train loss: 0.864
[3,   325] train loss: 0.864
[3,   330] train loss: 0.865
Finished Training
[3,     5] test loss: 0.857
[3,    10] test loss: 0.848
[3,    15] test loss: 0.861
[3,    20] test loss: 0.846
[3,    25] test loss: 0.853
[3,    30] test loss: 0.859
[3,    35] test loss: 0.841
[3,    40] test loss: 0.848
[3,    45] test loss: 0.841
[3,    50] test loss: 0.846
[3,    55] test loss: 0.860
[3,    60] test loss: 0.857
[3,    65] test loss: 0.839
[3,    70] test loss: 0.849
[3,    75] test loss: 0.850
[3,    80] test loss: 0.845
[3,    85] test loss: 0.862
[3,    90] test loss: 0.846
[3,    95] test loss: 0.845
[3,   100] test loss: 0.848
[3,   105] test loss: 0.839
[3,   110] test loss: 0.853
-----------------------------------------------------------------------------------------
Micro-Precision: 0.086943119764328, Macro-Precision: nan

Micro-Recall: 0.09494615346193314, Macro-Recall: nan

Micro-F1: 0.09076857566833496, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 133.58s | valid loss  0.86 | valid ppl     2.36
-----------------------------------------------------------------------------------------
[4,     5] train loss: 0.860
[4,    10] train loss: 0.858
[4,    15] train loss: 0.864
[4,    20] train loss: 0.845
[4,    25] train loss: 0.868
[4,    30] train loss: 0.868
[4,    35] train loss: 0.863
[4,    40] train loss: 0.856
[4,    45] train loss: 0.849
[4,    50] train loss: 0.858
[4,    55] train loss: 0.856
[4,    60] train loss: 0.865
[4,    65] train loss: 0.848
[4,    70] train loss: 0.858
[4,    75] train loss: 0.865
[4,    80] train loss: 0.848
[4,    85] train loss: 0.853
[4,    90] train loss: 0.863
[4,    95] train loss: 0.852
[4,   100] train loss: 0.854
[4,   105] train loss: 0.861
[4,   110] train loss: 0.853
[4,   115] train loss: 0.858
[4,   120] train loss: 0.848
[4,   125] train loss: 0.840
[4,   130] train loss: 0.856
[4,   135] train loss: 0.875
[4,   140] train loss: 0.856
[4,   145] train loss: 0.858
[4,   150] train loss: 0.852
[4,   155] train loss: 0.859
[4,   160] train loss: 0.861
[4,   165] train loss: 0.854
[4,   170] train loss: 0.854
[4,   175] train loss: 0.849
[4,   180] train loss: 0.848
[4,   185] train loss: 0.842
[4,   190] train loss: 0.849
[4,   195] train loss: 0.847
[4,   200] train loss: 0.833
[4,   205] train loss: 0.848
[4,   210] train loss: 0.849
[4,   215] train loss: 0.842
[4,   220] train loss: 0.839
[4,   225] train loss: 0.851
[4,   230] train loss: 0.849
[4,   235] train loss: 0.850
[4,   240] train loss: 0.849
[4,   245] train loss: 0.846
[4,   250] train loss: 0.849
[4,   255] train loss: 0.849
[4,   260] train loss: 0.843
[4,   265] train loss: 0.846
[4,   270] train loss: 0.840
[4,   275] train loss: 0.844
[4,   280] train loss: 0.841
[4,   285] train loss: 0.838
[4,   290] train loss: 0.840
[4,   295] train loss: 0.836
[4,   300] train loss: 0.838
[4,   305] train loss: 0.844
[4,   310] train loss: 0.839
[4,   315] train loss: 0.853
[4,   320] train loss: 0.849
[4,   325] train loss: 0.830
[4,   330] train loss: 0.842
Finished Training
[4,     5] test loss: 0.821
[4,    10] test loss: 0.819
[4,    15] test loss: 0.826
[4,    20] test loss: 0.820
[4,    25] test loss: 0.827
[4,    30] test loss: 0.826
[4,    35] test loss: 0.830
[4,    40] test loss: 0.822
[4,    45] test loss: 0.827
[4,    50] test loss: 0.818
[4,    55] test loss: 0.823
[4,    60] test loss: 0.823
[4,    65] test loss: 0.828
[4,    70] test loss: 0.832
[4,    75] test loss: 0.826
[4,    80] test loss: 0.835
[4,    85] test loss: 0.825
[4,    90] test loss: 0.830
[4,    95] test loss: 0.827
[4,   100] test loss: 0.825
[4,   105] test loss: 0.827
[4,   110] test loss: 0.823
-----------------------------------------------------------------------------------------
Micro-Precision: 0.12142152339220047, Macro-Precision: nan

Micro-Recall: 0.125978484749794, Macro-Recall: nan

Micro-F1: 0.12365803867578506, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 132.89s | valid loss  0.83 | valid ppl     2.30
-----------------------------------------------------------------------------------------
[5,     5] train loss: 0.850
[5,    10] train loss: 0.831
[5,    15] train loss: 0.834
[5,    20] train loss: 0.842
[5,    25] train loss: 0.831
[5,    30] train loss: 0.829
[5,    35] train loss: 0.845
[5,    40] train loss: 0.837
[5,    45] train loss: 0.840
[5,    50] train loss: 0.834
[5,    55] train loss: 0.828
[5,    60] train loss: 0.833
[5,    65] train loss: 0.838
[5,    70] train loss: 0.830
[5,    75] train loss: 0.838
[5,    80] train loss: 0.836
[5,    85] train loss: 0.840
[5,    90] train loss: 0.851
[5,    95] train loss: 0.841
[5,   100] train loss: 0.822
[5,   105] train loss: 0.834
[5,   110] train loss: 0.823
[5,   115] train loss: 0.840
[5,   120] train loss: 0.824
[5,   125] train loss: 0.834
[5,   130] train loss: 0.839
[5,   135] train loss: 0.847
[5,   140] train loss: 0.839
[5,   145] train loss: 0.839
[5,   150] train loss: 0.842
[5,   155] train loss: 0.835
[5,   160] train loss: 0.829
[5,   165] train loss: 0.839
[5,   170] train loss: 0.845
[5,   175] train loss: 0.836
[5,   180] train loss: 0.825
[5,   185] train loss: 0.822
[5,   190] train loss: 0.830
[5,   195] train loss: 0.828
[5,   200] train loss: 0.832
[5,   205] train loss: 0.822
[5,   210] train loss: 0.821
[5,   215] train loss: 0.828
[5,   220] train loss: 0.838
[5,   225] train loss: 0.835
[5,   230] train loss: 0.821
[5,   235] train loss: 0.819
[5,   240] train loss: 0.821
[5,   245] train loss: 0.828
[5,   250] train loss: 0.822
[5,   255] train loss: 0.827
[5,   260] train loss: 0.819
[5,   265] train loss: 0.828
[5,   270] train loss: 0.826
[5,   275] train loss: 0.828
[5,   280] train loss: 0.830
[5,   285] train loss: 0.824
[5,   290] train loss: 0.821
[5,   295] train loss: 0.821
[5,   300] train loss: 0.833
[5,   305] train loss: 0.837
[5,   310] train loss: 0.820
[5,   315] train loss: 0.824
[5,   320] train loss: 0.826
[5,   325] train loss: 0.823
[5,   330] train loss: 0.821
Finished Training
[5,     5] test loss: 0.807
[5,    10] test loss: 0.819
[5,    15] test loss: 0.814
[5,    20] test loss: 0.815
[5,    25] test loss: 0.803
[5,    30] test loss: 0.795
[5,    35] test loss: 0.810
[5,    40] test loss: 0.801
[5,    45] test loss: 0.806
[5,    50] test loss: 0.815
[5,    55] test loss: 0.811
[5,    60] test loss: 0.815
[5,    65] test loss: 0.802
[5,    70] test loss: 0.802
[5,    75] test loss: 0.801
[5,    80] test loss: 0.803
[5,    85] test loss: 0.799
[5,    90] test loss: 0.809
[5,    95] test loss: 0.812
[5,   100] test loss: 0.801
[5,   105] test loss: 0.807
[5,   110] test loss: 0.799
-----------------------------------------------------------------------------------------
Micro-Precision: 0.14930836856365204, Macro-Precision: nan

Micro-Recall: 0.16172361373901367, Macro-Recall: nan

Micro-F1: 0.15526820719242096, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 133.91s | valid loss  0.81 | valid ppl     2.26
-----------------------------------------------------------------------------------------
[6,     5] train loss: 0.839
[6,    10] train loss: 0.826
[6,    15] train loss: 0.825
[6,    20] train loss: 0.814
[6,    25] train loss: 0.827
[6,    30] train loss: 0.822
[6,    35] train loss: 0.823
[6,    40] train loss: 0.825
[6,    45] train loss: 0.831
[6,    50] train loss: 0.815
[6,    55] train loss: 0.824
[6,    60] train loss: 0.822
[6,    65] train loss: 0.816
[6,    70] train loss: 0.813
[6,    75] train loss: 0.827
[6,    80] train loss: 0.825
[6,    85] train loss: 0.835
[6,    90] train loss: 0.826
[6,    95] train loss: 0.808
[6,   100] train loss: 0.808
[6,   105] train loss: 0.821
[6,   110] train loss: 0.815
[6,   115] train loss: 0.820
[6,   120] train loss: 0.818
[6,   125] train loss: 0.809
[6,   130] train loss: 0.811
[6,   135] train loss: 0.811
[6,   140] train loss: 0.802
[6,   145] train loss: 0.812
[6,   150] train loss: 0.833
[6,   155] train loss: 0.817
[6,   160] train loss: 0.811
[6,   165] train loss: 0.817
[6,   170] train loss: 0.815
[6,   175] train loss: 0.788
[6,   180] train loss: 0.811
[6,   185] train loss: 0.800
[6,   190] train loss: 0.805
[6,   195] train loss: 0.817
[6,   200] train loss: 0.810
[6,   205] train loss: 0.809
[6,   210] train loss: 0.816
[6,   215] train loss: 0.822
[6,   220] train loss: 0.808
[6,   225] train loss: 0.819
[6,   230] train loss: 0.821
[6,   235] train loss: 0.818
[6,   240] train loss: 0.819
[6,   245] train loss: 0.820
[6,   250] train loss: 0.818
[6,   255] train loss: 0.818
[6,   260] train loss: 0.811
[6,   265] train loss: 0.806
[6,   270] train loss: 0.819
[6,   275] train loss: 0.816
[6,   280] train loss: 0.788
[6,   285] train loss: 0.821
[6,   290] train loss: 0.812
[6,   295] train loss: 0.828
[6,   300] train loss: 0.805
[6,   305] train loss: 0.807
[6,   310] train loss: 0.811
[6,   315] train loss: 0.817
[6,   320] train loss: 0.811
[6,   325] train loss: 0.796
[6,   330] train loss: 0.830
Finished Training
[6,     5] test loss: 0.788
[6,    10] test loss: 0.797
[6,    15] test loss: 0.786
[6,    20] test loss: 0.795
[6,    25] test loss: 0.793
[6,    30] test loss: 0.796
[6,    35] test loss: 0.786
[6,    40] test loss: 0.789
[6,    45] test loss: 0.795
[6,    50] test loss: 0.797
[6,    55] test loss: 0.795
[6,    60] test loss: 0.795
[6,    65] test loss: 0.784
[6,    70] test loss: 0.783
[6,    75] test loss: 0.798
[6,    80] test loss: 0.794
[6,    85] test loss: 0.813
[6,    90] test loss: 0.798
[6,    95] test loss: 0.799
[6,   100] test loss: 0.799
[6,   105] test loss: 0.784
[6,   110] test loss: 0.770
-----------------------------------------------------------------------------------------
Micro-Precision: 0.17207910120487213, Macro-Precision: nan

Micro-Recall: 0.1800304353237152, Macro-Recall: nan

Micro-F1: 0.17596499621868134, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 133.31s | valid loss  0.80 | valid ppl     2.22
-----------------------------------------------------------------------------------------
[7,     5] train loss: 0.815
[7,    10] train loss: 0.807
[7,    15] train loss: 0.796
[7,    20] train loss: 0.800
[7,    25] train loss: 0.810
[7,    30] train loss: 0.813
[7,    35] train loss: 0.798
[7,    40] train loss: 0.808
[7,    45] train loss: 0.809
[7,    50] train loss: 0.810
[7,    55] train loss: 0.796
[7,    60] train loss: 0.802
[7,    65] train loss: 0.820
[7,    70] train loss: 0.801
[7,    75] train loss: 0.801
[7,    80] train loss: 0.820
[7,    85] train loss: 0.799
[7,    90] train loss: 0.795
[7,    95] train loss: 0.799
[7,   100] train loss: 0.804
[7,   105] train loss: 0.801
[7,   110] train loss: 0.809
[7,   115] train loss: 0.808
[7,   120] train loss: 0.798
[7,   125] train loss: 0.808
[7,   130] train loss: 0.803
[7,   135] train loss: 0.814
[7,   140] train loss: 0.800
[7,   145] train loss: 0.797
[7,   150] train loss: 0.808
[7,   155] train loss: 0.803
[7,   160] train loss: 0.798
[7,   165] train loss: 0.810
[7,   170] train loss: 0.813
[7,   175] train loss: 0.811
[7,   180] train loss: 0.812
[7,   185] train loss: 0.804
[7,   190] train loss: 0.810
[7,   195] train loss: 0.811
[7,   200] train loss: 0.798
[7,   205] train loss: 0.807
[7,   210] train loss: 0.802
[7,   215] train loss: 0.787
[7,   220] train loss: 0.808
[7,   225] train loss: 0.805
[7,   230] train loss: 0.795
[7,   235] train loss: 0.799
[7,   240] train loss: 0.786
[7,   245] train loss: 0.805
[7,   250] train loss: 0.802
[7,   255] train loss: 0.803
[7,   260] train loss: 0.802
[7,   265] train loss: 0.814
[7,   270] train loss: 0.800
[7,   275] train loss: 0.792
[7,   280] train loss: 0.796
[7,   285] train loss: 0.797
[7,   290] train loss: 0.802
[7,   295] train loss: 0.810
[7,   300] train loss: 0.785
[7,   305] train loss: 0.804
[7,   310] train loss: 0.796
[7,   315] train loss: 0.790
[7,   320] train loss: 0.802
[7,   325] train loss: 0.794
[7,   330] train loss: 0.779
Finished Training
[7,     5] test loss: 0.766
[7,    10] test loss: 0.773
[7,    15] test loss: 0.779
[7,    20] test loss: 0.783
[7,    25] test loss: 0.787
[7,    30] test loss: 0.764
[7,    35] test loss: 0.799
[7,    40] test loss: 0.761
[7,    45] test loss: 0.790
[7,    50] test loss: 0.775
[7,    55] test loss: 0.780
[7,    60] test loss: 0.785
[7,    65] test loss: 0.784
[7,    70] test loss: 0.785
[7,    75] test loss: 0.774
[7,    80] test loss: 0.780
[7,    85] test loss: 0.800
[7,    90] test loss: 0.777
[7,    95] test loss: 0.782
[7,   100] test loss: 0.773
[7,   105] test loss: 0.784
[7,   110] test loss: 0.770
-----------------------------------------------------------------------------------------
Micro-Precision: 0.19108855724334717, Macro-Precision: nan

Micro-Recall: 0.2011149525642395, Macro-Recall: nan

Micro-F1: 0.19597359001636505, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 133.61s | valid loss  0.79 | valid ppl     2.20
-----------------------------------------------------------------------------------------
[8,     5] train loss: 0.793
[8,    10] train loss: 0.807
[8,    15] train loss: 0.793
[8,    20] train loss: 0.799
[8,    25] train loss: 0.783
[8,    30] train loss: 0.800
[8,    35] train loss: 0.793
[8,    40] train loss: 0.795
[8,    45] train loss: 0.802
[8,    50] train loss: 0.801
[8,    55] train loss: 0.795
[8,    60] train loss: 0.800
[8,    65] train loss: 0.798
[8,    70] train loss: 0.796
[8,    75] train loss: 0.806
[8,    80] train loss: 0.809
[8,    85] train loss: 0.793
[8,    90] train loss: 0.790
[8,    95] train loss: 0.799
[8,   100] train loss: 0.788
[8,   105] train loss: 0.786
[8,   110] train loss: 0.786
[8,   115] train loss: 0.791
[8,   120] train loss: 0.786
[8,   125] train loss: 0.786
[8,   130] train loss: 0.789
[8,   135] train loss: 0.803
[8,   140] train loss: 0.786
[8,   145] train loss: 0.784
[8,   150] train loss: 0.784
[8,   155] train loss: 0.803
[8,   160] train loss: 0.794
[8,   165] train loss: 0.793
[8,   170] train loss: 0.788
[8,   175] train loss: 0.785
[8,   180] train loss: 0.783
[8,   185] train loss: 0.803
[8,   190] train loss: 0.788
[8,   195] train loss: 0.808
[8,   200] train loss: 0.784
[8,   205] train loss: 0.786
[8,   210] train loss: 0.794
[8,   215] train loss: 0.783
[8,   220] train loss: 0.793
[8,   225] train loss: 0.793
[8,   230] train loss: 0.792
[8,   235] train loss: 0.785
[8,   240] train loss: 0.789
[8,   245] train loss: 0.788
[8,   250] train loss: 0.791
[8,   255] train loss: 0.784
[8,   260] train loss: 0.784
[8,   265] train loss: 0.783
[8,   270] train loss: 0.781
[8,   275] train loss: 0.782
[8,   280] train loss: 0.799
[8,   285] train loss: 0.794
[8,   290] train loss: 0.777
[8,   295] train loss: 0.803
[8,   300] train loss: 0.781
[8,   305] train loss: 0.783
[8,   310] train loss: 0.776
[8,   315] train loss: 0.786
[8,   320] train loss: 0.783
[8,   325] train loss: 0.793
[8,   330] train loss: 0.783
Finished Training
[8,     5] test loss: 0.765
[8,    10] test loss: 0.768
[8,    15] test loss: 0.750
[8,    20] test loss: 0.769
[8,    25] test loss: 0.785
[8,    30] test loss: 0.785
[8,    35] test loss: 0.781
[8,    40] test loss: 0.773
[8,    45] test loss: 0.771
[8,    50] test loss: 0.784
[8,    55] test loss: 0.759
[8,    60] test loss: 0.769
[8,    65] test loss: 0.768
[8,    70] test loss: 0.754
[8,    75] test loss: 0.771
[8,    80] test loss: 0.769
[8,    85] test loss: 0.766
[8,    90] test loss: 0.765
[8,    95] test loss: 0.776
[8,   100] test loss: 0.758
[8,   105] test loss: 0.775
[8,   110] test loss: 0.769
-----------------------------------------------------------------------------------------
Micro-Precision: 0.20014195144176483, Macro-Precision: nan

Micro-Recall: 0.2227993607521057, Macro-Recall: nan

Micro-F1: 0.21086376905441284, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 134.57s | valid loss  0.78 | valid ppl     2.17
-----------------------------------------------------------------------------------------
[9,     5] train loss: 0.776
[9,    10] train loss: 0.776
[9,    15] train loss: 0.800
[9,    20] train loss: 0.792
[9,    25] train loss: 0.786
[9,    30] train loss: 0.786
[9,    35] train loss: 0.784
[9,    40] train loss: 0.792
[9,    45] train loss: 0.774
[9,    50] train loss: 0.784
[9,    55] train loss: 0.775
[9,    60] train loss: 0.797
[9,    65] train loss: 0.783
[9,    70] train loss: 0.794
[9,    75] train loss: 0.772
[9,    80] train loss: 0.793
[9,    85] train loss: 0.784
[9,    90] train loss: 0.787
[9,    95] train loss: 0.792
[9,   100] train loss: 0.787
[9,   105] train loss: 0.779
[9,   110] train loss: 0.794
[9,   115] train loss: 0.778
[9,   120] train loss: 0.778
[9,   125] train loss: 0.778
[9,   130] train loss: 0.790
[9,   135] train loss: 0.795
[9,   140] train loss: 0.782
[9,   145] train loss: 0.773
[9,   150] train loss: 0.786
[9,   155] train loss: 0.798
[9,   160] train loss: 0.770
[9,   165] train loss: 0.783
[9,   170] train loss: 0.777
[9,   175] train loss: 0.795
[9,   180] train loss: 0.772
[9,   185] train loss: 0.776
[9,   190] train loss: 0.781
[9,   195] train loss: 0.787
[9,   200] train loss: 0.768
[9,   205] train loss: 0.779
[9,   210] train loss: 0.783
[9,   215] train loss: 0.780
[9,   220] train loss: 0.783
[9,   225] train loss: 0.784
[9,   230] train loss: 0.772
[9,   235] train loss: 0.784
[9,   240] train loss: 0.767
[9,   245] train loss: 0.775
[9,   250] train loss: 0.762
[9,   255] train loss: 0.777
[9,   260] train loss: 0.787
[9,   265] train loss: 0.765
[9,   270] train loss: 0.782
[9,   275] train loss: 0.784
[9,   280] train loss: 0.770
[9,   285] train loss: 0.779
[9,   290] train loss: 0.778
[9,   295] train loss: 0.779
[9,   300] train loss: 0.783
[9,   305] train loss: 0.773
[9,   310] train loss: 0.769
[9,   315] train loss: 0.791
[9,   320] train loss: 0.786
[9,   325] train loss: 0.778
[9,   330] train loss: 0.790
Finished Training
[9,     5] test loss: 0.753
[9,    10] test loss: 0.759
[9,    15] test loss: 0.758
[9,    20] test loss: 0.756
[9,    25] test loss: 0.759
[9,    30] test loss: 0.750
[9,    35] test loss: 0.768
[9,    40] test loss: 0.760
[9,    45] test loss: 0.757
[9,    50] test loss: 0.768
[9,    55] test loss: 0.754
[9,    60] test loss: 0.749
[9,    65] test loss: 0.756
[9,    70] test loss: 0.758
[9,    75] test loss: 0.775
[9,    80] test loss: 0.757
[9,    85] test loss: 0.767
[9,    90] test loss: 0.773
[9,    95] test loss: 0.765
[9,   100] test loss: 0.753
[9,   105] test loss: 0.772
[9,   110] test loss: 0.745
-----------------------------------------------------------------------------------------
Micro-Precision: 0.21199244260787964, Macro-Precision: nan

Micro-Recall: 0.23138827085494995, Macro-Recall: nan

Micro-F1: 0.22126612067222595, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 134.52s | valid loss  0.77 | valid ppl     2.15
-----------------------------------------------------------------------------------------
[10,     5] train loss: 0.775
[10,    10] train loss: 0.763
[10,    15] train loss: 0.779
[10,    20] train loss: 0.783
[10,    25] train loss: 0.776
[10,    30] train loss: 0.785
[10,    35] train loss: 0.779
[10,    40] train loss: 0.789
[10,    45] train loss: 0.769
[10,    50] train loss: 0.786
[10,    55] train loss: 0.772
[10,    60] train loss: 0.774
[10,    65] train loss: 0.766
[10,    70] train loss: 0.780
[10,    75] train loss: 0.790
[10,    80] train loss: 0.768
[10,    85] train loss: 0.777
[10,    90] train loss: 0.759
[10,    95] train loss: 0.773
[10,   100] train loss: 0.765
[10,   105] train loss: 0.766
[10,   110] train loss: 0.796
[10,   115] train loss: 0.763
[10,   120] train loss: 0.758
[10,   125] train loss: 0.770
[10,   130] train loss: 0.765
[10,   135] train loss: 0.772
[10,   140] train loss: 0.771
[10,   145] train loss: 0.791
[10,   150] train loss: 0.755
[10,   155] train loss: 0.786
[10,   160] train loss: 0.775
[10,   165] train loss: 0.777
[10,   170] train loss: 0.766
[10,   175] train loss: 0.769
[10,   180] train loss: 0.776
[10,   185] train loss: 0.769
[10,   190] train loss: 0.765
[10,   195] train loss: 0.782
[10,   200] train loss: 0.777
[10,   205] train loss: 0.765
[10,   210] train loss: 0.777
[10,   215] train loss: 0.767
[10,   220] train loss: 0.772
[10,   225] train loss: 0.765
[10,   230] train loss: 0.769
[10,   235] train loss: 0.762
[10,   240] train loss: 0.769
[10,   245] train loss: 0.781
[10,   250] train loss: 0.765
[10,   255] train loss: 0.767
[10,   260] train loss: 0.771
[10,   265] train loss: 0.753
[10,   270] train loss: 0.773
[10,   275] train loss: 0.779
[10,   280] train loss: 0.768
[10,   285] train loss: 0.776
[10,   290] train loss: 0.785
[10,   295] train loss: 0.759
[10,   300] train loss: 0.764
[10,   305] train loss: 0.763
[10,   310] train loss: 0.778
[10,   315] train loss: 0.762
[10,   320] train loss: 0.773
[10,   325] train loss: 0.770
[10,   330] train loss: 0.767
Finished Training
[10,     5] test loss: 0.754
[10,    10] test loss: 0.757
[10,    15] test loss: 0.774
[10,    20] test loss: 0.739
[10,    25] test loss: 0.754
[10,    30] test loss: 0.742
[10,    35] test loss: 0.745
[10,    40] test loss: 0.765
[10,    45] test loss: 0.747
[10,    50] test loss: 0.763
[10,    55] test loss: 0.736
[10,    60] test loss: 0.734
[10,    65] test loss: 0.756
[10,    70] test loss: 0.739
[10,    75] test loss: 0.738
[10,    80] test loss: 0.751
[10,    85] test loss: 0.750
[10,    90] test loss: 0.753
[10,    95] test loss: 0.754
[10,   100] test loss: 0.766
[10,   105] test loss: 0.746
[10,   110] test loss: 0.759
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2234330177307129, Macro-Precision: nan

Micro-Recall: 0.2442057877779007, Macro-Recall: nan

Micro-F1: 0.23335804045200348, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 134.57s | valid loss  0.76 | valid ppl     2.13
-----------------------------------------------------------------------------------------
[11,     5] train loss: 0.756
[11,    10] train loss: 0.779
[11,    15] train loss: 0.778
[11,    20] train loss: 0.751
[11,    25] train loss: 0.761
[11,    30] train loss: 0.771
[11,    35] train loss: 0.762
[11,    40] train loss: 0.758
[11,    45] train loss: 0.776
[11,    50] train loss: 0.774
[11,    55] train loss: 0.747
[11,    60] train loss: 0.776
[11,    65] train loss: 0.766
[11,    70] train loss: 0.777
[11,    75] train loss: 0.761
[11,    80] train loss: 0.765
[11,    85] train loss: 0.766
[11,    90] train loss: 0.762
[11,    95] train loss: 0.767
[11,   100] train loss: 0.756
[11,   105] train loss: 0.751
[11,   110] train loss: 0.766
[11,   115] train loss: 0.745
[11,   120] train loss: 0.769
[11,   125] train loss: 0.762
[11,   130] train loss: 0.765
[11,   135] train loss: 0.779
[11,   140] train loss: 0.759
[11,   145] train loss: 0.752
[11,   150] train loss: 0.762
[11,   155] train loss: 0.758
[11,   160] train loss: 0.776
[11,   165] train loss: 0.760
[11,   170] train loss: 0.783
[11,   175] train loss: 0.772
[11,   180] train loss: 0.778
[11,   185] train loss: 0.765
[11,   190] train loss: 0.773
[11,   195] train loss: 0.754
[11,   200] train loss: 0.764
[11,   205] train loss: 0.781
[11,   210] train loss: 0.759
[11,   215] train loss: 0.766
[11,   220] train loss: 0.753
[11,   225] train loss: 0.753
[11,   230] train loss: 0.762
[11,   235] train loss: 0.763
[11,   240] train loss: 0.769
[11,   245] train loss: 0.747
[11,   250] train loss: 0.756
[11,   255] train loss: 0.756
[11,   260] train loss: 0.769
[11,   265] train loss: 0.748
[11,   270] train loss: 0.771
[11,   275] train loss: 0.763
[11,   280] train loss: 0.772
[11,   285] train loss: 0.762
[11,   290] train loss: 0.758
[11,   295] train loss: 0.772
[11,   300] train loss: 0.757
[11,   305] train loss: 0.763
[11,   310] train loss: 0.762
[11,   315] train loss: 0.773
[11,   320] train loss: 0.754
[11,   325] train loss: 0.761
[11,   330] train loss: 0.741
Finished Training
[11,     5] test loss: 0.742
[11,    10] test loss: 0.739
[11,    15] test loss: 0.736
[11,    20] test loss: 0.750
[11,    25] test loss: 0.737
[11,    30] test loss: 0.740
[11,    35] test loss: 0.733
[11,    40] test loss: 0.732
[11,    45] test loss: 0.758
[11,    50] test loss: 0.728
[11,    55] test loss: 0.740
[11,    60] test loss: 0.747
[11,    65] test loss: 0.749
[11,    70] test loss: 0.768
[11,    75] test loss: 0.740
[11,    80] test loss: 0.739
[11,    85] test loss: 0.755
[11,    90] test loss: 0.754
[11,    95] test loss: 0.759
[11,   100] test loss: 0.744
[11,   105] test loss: 0.728
[11,   110] test loss: 0.742
-----------------------------------------------------------------------------------------
Micro-Precision: 0.22996622323989868, Macro-Precision: nan

Micro-Recall: 0.25399449467658997, Macro-Recall: nan

Micro-F1: 0.2413838654756546, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 134.84s | valid loss  0.75 | valid ppl     2.12
-----------------------------------------------------------------------------------------
[12,     5] train loss: 0.761
[12,    10] train loss: 0.755
[12,    15] train loss: 0.753
[12,    20] train loss: 0.762
[12,    25] train loss: 0.762
[12,    30] train loss: 0.757
[12,    35] train loss: 0.754
[12,    40] train loss: 0.763
[12,    45] train loss: 0.758
[12,    50] train loss: 0.748
[12,    55] train loss: 0.757
[12,    60] train loss: 0.752
[12,    65] train loss: 0.761
[12,    70] train loss: 0.761
[12,    75] train loss: 0.764
[12,    80] train loss: 0.751
[12,    85] train loss: 0.748
[12,    90] train loss: 0.738
[12,    95] train loss: 0.756
[12,   100] train loss: 0.759
[12,   105] train loss: 0.753
[12,   110] train loss: 0.771
[12,   115] train loss: 0.748
[12,   120] train loss: 0.744
[12,   125] train loss: 0.751
[12,   130] train loss: 0.757
[12,   135] train loss: 0.751
[12,   140] train loss: 0.762
[12,   145] train loss: 0.747
[12,   150] train loss: 0.770
[12,   155] train loss: 0.745
[12,   160] train loss: 0.754
[12,   165] train loss: 0.751
[12,   170] train loss: 0.753
[12,   175] train loss: 0.739
[12,   180] train loss: 0.764
[12,   185] train loss: 0.756
[12,   190] train loss: 0.750
[12,   195] train loss: 0.755
[12,   200] train loss: 0.772
[12,   205] train loss: 0.752
[12,   210] train loss: 0.746
[12,   215] train loss: 0.760
[12,   220] train loss: 0.757
[12,   225] train loss: 0.763
[12,   230] train loss: 0.756
[12,   235] train loss: 0.764
[12,   240] train loss: 0.762
[12,   245] train loss: 0.772
[12,   250] train loss: 0.738
[12,   255] train loss: 0.762
[12,   260] train loss: 0.751
[12,   265] train loss: 0.756
[12,   270] train loss: 0.748
[12,   275] train loss: 0.745
[12,   280] train loss: 0.752
[12,   285] train loss: 0.754
[12,   290] train loss: 0.758
[12,   295] train loss: 0.766
[12,   300] train loss: 0.754
[12,   305] train loss: 0.748
[12,   310] train loss: 0.771
[12,   315] train loss: 0.752
[12,   320] train loss: 0.753
[12,   325] train loss: 0.763
[12,   330] train loss: 0.768
Finished Training
[12,     5] test loss: 0.728
[12,    10] test loss: 0.738
[12,    15] test loss: 0.721
[12,    20] test loss: 0.721
[12,    25] test loss: 0.747
[12,    30] test loss: 0.738
[12,    35] test loss: 0.759
[12,    40] test loss: 0.741
[12,    45] test loss: 0.739
[12,    50] test loss: 0.757
[12,    55] test loss: 0.735
[12,    60] test loss: 0.719
[12,    65] test loss: 0.732
[12,    70] test loss: 0.745
[12,    75] test loss: 0.743
[12,    80] test loss: 0.734
[12,    85] test loss: 0.750
[12,    90] test loss: 0.747
[12,    95] test loss: 0.737
[12,   100] test loss: 0.719
[12,   105] test loss: 0.734
[12,   110] test loss: 0.732
-----------------------------------------------------------------------------------------
Micro-Precision: 0.23941905796527863, Macro-Precision: nan

Micro-Recall: 0.25567716360092163, Macro-Recall: nan

Micro-F1: 0.24728116393089294, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 134.17s | valid loss  0.74 | valid ppl     2.10
-----------------------------------------------------------------------------------------
[13,     5] train loss: 0.744
[13,    10] train loss: 0.757
[13,    15] train loss: 0.728
[13,    20] train loss: 0.749
[13,    25] train loss: 0.747
[13,    30] train loss: 0.755
[13,    35] train loss: 0.753
[13,    40] train loss: 0.753
[13,    45] train loss: 0.748
[13,    50] train loss: 0.753
[13,    55] train loss: 0.758
[13,    60] train loss: 0.753
[13,    65] train loss: 0.742
[13,    70] train loss: 0.741
[13,    75] train loss: 0.752
[13,    80] train loss: 0.729
[13,    85] train loss: 0.750
[13,    90] train loss: 0.737
[13,    95] train loss: 0.749
[13,   100] train loss: 0.744
[13,   105] train loss: 0.748
[13,   110] train loss: 0.744
[13,   115] train loss: 0.760
[13,   120] train loss: 0.739
[13,   125] train loss: 0.744
[13,   130] train loss: 0.746
[13,   135] train loss: 0.764
[13,   140] train loss: 0.754
[13,   145] train loss: 0.745
[13,   150] train loss: 0.752
[13,   155] train loss: 0.752
[13,   160] train loss: 0.744
[13,   165] train loss: 0.753
[13,   170] train loss: 0.763
[13,   175] train loss: 0.737
[13,   180] train loss: 0.752
[13,   185] train loss: 0.758
[13,   190] train loss: 0.736
[13,   195] train loss: 0.732
[13,   200] train loss: 0.742
[13,   205] train loss: 0.749
[13,   210] train loss: 0.759
[13,   215] train loss: 0.736
[13,   220] train loss: 0.759
[13,   225] train loss: 0.742
[13,   230] train loss: 0.767
[13,   235] train loss: 0.740
[13,   240] train loss: 0.753
[13,   245] train loss: 0.752
[13,   250] train loss: 0.745
[13,   255] train loss: 0.737
[13,   260] train loss: 0.754
[13,   265] train loss: 0.731
[13,   270] train loss: 0.752
[13,   275] train loss: 0.763
[13,   280] train loss: 0.741
[13,   285] train loss: 0.745
[13,   290] train loss: 0.760
[13,   295] train loss: 0.752
[13,   300] train loss: 0.742
[13,   305] train loss: 0.765
[13,   310] train loss: 0.741
[13,   315] train loss: 0.753
[13,   320] train loss: 0.738
[13,   325] train loss: 0.767
[13,   330] train loss: 0.740
Finished Training
[13,     5] test loss: 0.717
[13,    10] test loss: 0.727
[13,    15] test loss: 0.730
[13,    20] test loss: 0.735
[13,    25] test loss: 0.716
[13,    30] test loss: 0.728
[13,    35] test loss: 0.726
[13,    40] test loss: 0.749
[13,    45] test loss: 0.741
[13,    50] test loss: 0.746
[13,    55] test loss: 0.726
[13,    60] test loss: 0.738
[13,    65] test loss: 0.729
[13,    70] test loss: 0.716
[13,    75] test loss: 0.706
[13,    80] test loss: 0.732
[13,    85] test loss: 0.732
[13,    90] test loss: 0.734
[13,    95] test loss: 0.731
[13,   100] test loss: 0.726
[13,   105] test loss: 0.760
[13,   110] test loss: 0.724
-----------------------------------------------------------------------------------------
Micro-Precision: 0.245441734790802, Macro-Precision: nan

Micro-Recall: 0.2625541388988495, Macro-Recall: nan

Micro-F1: 0.25370970368385315, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  12 | time: 134.27s | valid loss  0.74 | valid ppl     2.09
-----------------------------------------------------------------------------------------
[14,     5] train loss: 0.733
[14,    10] train loss: 0.736
[14,    15] train loss: 0.741
[14,    20] train loss: 0.743
[14,    25] train loss: 0.732
[14,    30] train loss: 0.766
[14,    35] train loss: 0.739
[14,    40] train loss: 0.742
[14,    45] train loss: 0.738
[14,    50] train loss: 0.749
[14,    55] train loss: 0.736
[14,    60] train loss: 0.749
[14,    65] train loss: 0.733
[14,    70] train loss: 0.737
[14,    75] train loss: 0.745
[14,    80] train loss: 0.738
[14,    85] train loss: 0.737
[14,    90] train loss: 0.737
[14,    95] train loss: 0.740
[14,   100] train loss: 0.725
[14,   105] train loss: 0.756
[14,   110] train loss: 0.758
[14,   115] train loss: 0.748
[14,   120] train loss: 0.743
[14,   125] train loss: 0.747
[14,   130] train loss: 0.750
[14,   135] train loss: 0.753
[14,   140] train loss: 0.745
[14,   145] train loss: 0.746
[14,   150] train loss: 0.748
[14,   155] train loss: 0.747
[14,   160] train loss: 0.745
[14,   165] train loss: 0.756
[14,   170] train loss: 0.725
[14,   175] train loss: 0.747
[14,   180] train loss: 0.749
[14,   185] train loss: 0.745
[14,   190] train loss: 0.740
[14,   195] train loss: 0.737
[14,   200] train loss: 0.721
[14,   205] train loss: 0.735
[14,   210] train loss: 0.724
[14,   215] train loss: 0.747
[14,   220] train loss: 0.731
[14,   225] train loss: 0.742
[14,   230] train loss: 0.735
[14,   235] train loss: 0.745
[14,   240] train loss: 0.730
[14,   245] train loss: 0.742
[14,   250] train loss: 0.752
[14,   255] train loss: 0.744
[14,   260] train loss: 0.743
[14,   265] train loss: 0.746
[14,   270] train loss: 0.732
[14,   275] train loss: 0.736
[14,   280] train loss: 0.742
[14,   285] train loss: 0.737
[14,   290] train loss: 0.740
[14,   295] train loss: 0.743
[14,   300] train loss: 0.745
[14,   305] train loss: 0.738
[14,   310] train loss: 0.755
[14,   315] train loss: 0.728
[14,   320] train loss: 0.730
[14,   325] train loss: 0.733
[14,   330] train loss: 0.751
Finished Training
[14,     5] test loss: 0.725
[14,    10] test loss: 0.720
[14,    15] test loss: 0.728
[14,    20] test loss: 0.719
[14,    25] test loss: 0.708
[14,    30] test loss: 0.745
[14,    35] test loss: 0.716
[14,    40] test loss: 0.718
[14,    45] test loss: 0.726
[14,    50] test loss: 0.737
[14,    55] test loss: 0.728
[14,    60] test loss: 0.724
[14,    65] test loss: 0.734
[14,    70] test loss: 0.725
[14,    75] test loss: 0.726
[14,    80] test loss: 0.717
[14,    85] test loss: 0.738
[14,    90] test loss: 0.727
[14,    95] test loss: 0.732
[14,   100] test loss: 0.721
[14,   105] test loss: 0.701
[14,   110] test loss: 0.732
-----------------------------------------------------------------------------------------
Micro-Precision: 0.25195521116256714, Macro-Precision: nan

Micro-Recall: 0.27057239413261414, Macro-Recall: nan

Micro-F1: 0.26093214750289917, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  13 | time: 134.45s | valid loss  0.73 | valid ppl     2.08
-----------------------------------------------------------------------------------------
[15,     5] train loss: 0.736
[15,    10] train loss: 0.739
[15,    15] train loss: 0.758
[15,    20] train loss: 0.731
[15,    25] train loss: 0.738
[15,    30] train loss: 0.728
[15,    35] train loss: 0.721
[15,    40] train loss: 0.730
[15,    45] train loss: 0.736
[15,    50] train loss: 0.741
[15,    55] train loss: 0.732
[15,    60] train loss: 0.718
[15,    65] train loss: 0.745
[15,    70] train loss: 0.751
[15,    75] train loss: 0.731
[15,    80] train loss: 0.728
[15,    85] train loss: 0.741
[15,    90] train loss: 0.759
[15,    95] train loss: 0.733
[15,   100] train loss: 0.710
[15,   105] train loss: 0.725
[15,   110] train loss: 0.747
[15,   115] train loss: 0.737
[15,   120] train loss: 0.728
[15,   125] train loss: 0.733
[15,   130] train loss: 0.729
[15,   135] train loss: 0.728
[15,   140] train loss: 0.731
[15,   145] train loss: 0.730
[15,   150] train loss: 0.731
[15,   155] train loss: 0.724
[15,   160] train loss: 0.733
[15,   165] train loss: 0.727
[15,   170] train loss: 0.745
[15,   175] train loss: 0.733
[15,   180] train loss: 0.726
[15,   185] train loss: 0.738
[15,   190] train loss: 0.729
[15,   195] train loss: 0.736
[15,   200] train loss: 0.730
[15,   205] train loss: 0.750
[15,   210] train loss: 0.747
[15,   215] train loss: 0.739
[15,   220] train loss: 0.734
[15,   225] train loss: 0.731
[15,   230] train loss: 0.730
[15,   235] train loss: 0.732
[15,   240] train loss: 0.731
[15,   245] train loss: 0.725
[15,   250] train loss: 0.719
[15,   255] train loss: 0.744
[15,   260] train loss: 0.729
[15,   265] train loss: 0.750
[15,   270] train loss: 0.731
[15,   275] train loss: 0.732
[15,   280] train loss: 0.735
[15,   285] train loss: 0.741
[15,   290] train loss: 0.732
[15,   295] train loss: 0.733
[15,   300] train loss: 0.745
[15,   305] train loss: 0.745
[15,   310] train loss: 0.749
[15,   315] train loss: 0.731
[15,   320] train loss: 0.733
[15,   325] train loss: 0.729
[15,   330] train loss: 0.749
Finished Training
[15,     5] test loss: 0.723
[15,    10] test loss: 0.734
[15,    15] test loss: 0.733
[15,    20] test loss: 0.718
[15,    25] test loss: 0.714
[15,    30] test loss: 0.710
[15,    35] test loss: 0.715
[15,    40] test loss: 0.715
[15,    45] test loss: 0.716
[15,    50] test loss: 0.731
[15,    55] test loss: 0.737
[15,    60] test loss: 0.725
[15,    65] test loss: 0.734
[15,    70] test loss: 0.707
[15,    75] test loss: 0.712
[15,    80] test loss: 0.716
[15,    85] test loss: 0.712
[15,    90] test loss: 0.719
[15,    95] test loss: 0.721
[15,   100] test loss: 0.716
[15,   105] test loss: 0.730
[15,   110] test loss: 0.694
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2570474147796631, Macro-Precision: nan

Micro-Recall: 0.27244529128074646, Macro-Recall: nan

Micro-F1: 0.2645224630832672, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  14 | time: 134.20s | valid loss  0.73 | valid ppl     2.07
-----------------------------------------------------------------------------------------
[16,     5] train loss: 0.734
[16,    10] train loss: 0.725
[16,    15] train loss: 0.729
[16,    20] train loss: 0.729
[16,    25] train loss: 0.733
[16,    30] train loss: 0.732
[16,    35] train loss: 0.731
[16,    40] train loss: 0.736
[16,    45] train loss: 0.731
[16,    50] train loss: 0.728
[16,    55] train loss: 0.730
[16,    60] train loss: 0.727
[16,    65] train loss: 0.738
[16,    70] train loss: 0.719
[16,    75] train loss: 0.726
[16,    80] train loss: 0.730
[16,    85] train loss: 0.729
[16,    90] train loss: 0.717
[16,    95] train loss: 0.719
[16,   100] train loss: 0.725
[16,   105] train loss: 0.723
[16,   110] train loss: 0.744
[16,   115] train loss: 0.716
[16,   120] train loss: 0.726
[16,   125] train loss: 0.728
[16,   130] train loss: 0.737
[16,   135] train loss: 0.731
[16,   140] train loss: 0.732
[16,   145] train loss: 0.723
[16,   150] train loss: 0.732
[16,   155] train loss: 0.732
[16,   160] train loss: 0.740
[16,   165] train loss: 0.734
[16,   170] train loss: 0.737
[16,   175] train loss: 0.727
[16,   180] train loss: 0.726
[16,   185] train loss: 0.731
[16,   190] train loss: 0.729
[16,   195] train loss: 0.745
[16,   200] train loss: 0.728
[16,   205] train loss: 0.712
[16,   210] train loss: 0.730
[16,   215] train loss: 0.729
[16,   220] train loss: 0.735
[16,   225] train loss: 0.735
[16,   230] train loss: 0.718
[16,   235] train loss: 0.715
[16,   240] train loss: 0.724
[16,   245] train loss: 0.723
[16,   250] train loss: 0.721
[16,   255] train loss: 0.717
[16,   260] train loss: 0.716
[16,   265] train loss: 0.736
[16,   270] train loss: 0.755
[16,   275] train loss: 0.729
[16,   280] train loss: 0.732
[16,   285] train loss: 0.729
[16,   290] train loss: 0.726
[16,   295] train loss: 0.720
[16,   300] train loss: 0.726
[16,   305] train loss: 0.731
[16,   310] train loss: 0.731
[16,   315] train loss: 0.719
[16,   320] train loss: 0.723
[16,   325] train loss: 0.726
[16,   330] train loss: 0.735
Finished Training
[16,     5] test loss: 0.742
[16,    10] test loss: 0.723
[16,    15] test loss: 0.725
[16,    20] test loss: 0.709
[16,    25] test loss: 0.716
[16,    30] test loss: 0.719
[16,    35] test loss: 0.701
[16,    40] test loss: 0.706
[16,    45] test loss: 0.714
[16,    50] test loss: 0.701
[16,    55] test loss: 0.726
[16,    60] test loss: 0.707
[16,    65] test loss: 0.722
[16,    70] test loss: 0.704
[16,    75] test loss: 0.717
[16,    80] test loss: 0.713
[16,    85] test loss: 0.714
[16,    90] test loss: 0.697
[16,    95] test loss: 0.713
[16,   100] test loss: 0.722
[16,   105] test loss: 0.726
[16,   110] test loss: 0.718
-----------------------------------------------------------------------------------------
Micro-Precision: 0.26328352093696594, Macro-Precision: nan

Micro-Recall: 0.2856620252132416, Macro-Recall: nan

Micro-F1: 0.2740166187286377, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  15 | time: 134.63s | valid loss  0.72 | valid ppl     2.06
-----------------------------------------------------------------------------------------
[17,     5] train loss: 0.725
[17,    10] train loss: 0.740
[17,    15] train loss: 0.732
[17,    20] train loss: 0.744
[17,    25] train loss: 0.702
[17,    30] train loss: 0.718
[17,    35] train loss: 0.717
[17,    40] train loss: 0.732
[17,    45] train loss: 0.721
[17,    50] train loss: 0.732
[17,    55] train loss: 0.730
[17,    60] train loss: 0.725
[17,    65] train loss: 0.707
[17,    70] train loss: 0.718
[17,    75] train loss: 0.704
[17,    80] train loss: 0.725
[17,    85] train loss: 0.716
[17,    90] train loss: 0.722
[17,    95] train loss: 0.714
[17,   100] train loss: 0.718
[17,   105] train loss: 0.723
[17,   110] train loss: 0.716
[17,   115] train loss: 0.738
[17,   120] train loss: 0.737
[17,   125] train loss: 0.710
[17,   130] train loss: 0.712
[17,   135] train loss: 0.723
[17,   140] train loss: 0.716
[17,   145] train loss: 0.723
[17,   150] train loss: 0.730
[17,   155] train loss: 0.712
[17,   160] train loss: 0.737
[17,   165] train loss: 0.728
[17,   170] train loss: 0.737
[17,   175] train loss: 0.724
[17,   180] train loss: 0.725
[17,   185] train loss: 0.726
[17,   190] train loss: 0.715
[17,   195] train loss: 0.708
[17,   200] train loss: 0.714
[17,   205] train loss: 0.742
[17,   210] train loss: 0.724
[17,   215] train loss: 0.726
[17,   220] train loss: 0.730
[17,   225] train loss: 0.729
[17,   230] train loss: 0.728
[17,   235] train loss: 0.715
[17,   240] train loss: 0.716
[17,   245] train loss: 0.723
[17,   250] train loss: 0.725
[17,   255] train loss: 0.749
[17,   260] train loss: 0.740
[17,   265] train loss: 0.722
[17,   270] train loss: 0.724
[17,   275] train loss: 0.732
[17,   280] train loss: 0.712
[17,   285] train loss: 0.731
[17,   290] train loss: 0.723
[17,   295] train loss: 0.716
[17,   300] train loss: 0.714
[17,   305] train loss: 0.713
[17,   310] train loss: 0.715
[17,   315] train loss: 0.723
[17,   320] train loss: 0.724
[17,   325] train loss: 0.726
[17,   330] train loss: 0.711
Finished Training
[17,     5] test loss: 0.689
[17,    10] test loss: 0.714
[17,    15] test loss: 0.707
[17,    20] test loss: 0.711
[17,    25] test loss: 0.707
[17,    30] test loss: 0.694
[17,    35] test loss: 0.707
[17,    40] test loss: 0.704
[17,    45] test loss: 0.725
[17,    50] test loss: 0.707
[17,    55] test loss: 0.719
[17,    60] test loss: 0.722
[17,    65] test loss: 0.712
[17,    70] test loss: 0.706
[17,    75] test loss: 0.723
[17,    80] test loss: 0.704
[17,    85] test loss: 0.683
[17,    90] test loss: 0.712
[17,    95] test loss: 0.719
[17,   100] test loss: 0.723
[17,   105] test loss: 0.729
[17,   110] test loss: 0.708
-----------------------------------------------------------------------------------------
Micro-Precision: 0.26691338419914246, Macro-Precision: nan

Micro-Recall: 0.2891590893268585, Macro-Recall: nan

Micro-F1: 0.2775912582874298, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  16 | time: 134.73s | valid loss  0.72 | valid ppl     2.05
-----------------------------------------------------------------------------------------
[18,     5] train loss: 0.726
[18,    10] train loss: 0.743
[18,    15] train loss: 0.718
[18,    20] train loss: 0.726
[18,    25] train loss: 0.712
[18,    30] train loss: 0.728
[18,    35] train loss: 0.711
[18,    40] train loss: 0.731
[18,    45] train loss: 0.723
[18,    50] train loss: 0.713
[18,    55] train loss: 0.719
[18,    60] train loss: 0.734
[18,    65] train loss: 0.703
[18,    70] train loss: 0.716
[18,    75] train loss: 0.711
[18,    80] train loss: 0.721
[18,    85] train loss: 0.726
[18,    90] train loss: 0.705
[18,    95] train loss: 0.722
[18,   100] train loss: 0.720
[18,   105] train loss: 0.711
[18,   110] train loss: 0.715
[18,   115] train loss: 0.721
[18,   120] train loss: 0.726
[18,   125] train loss: 0.708
[18,   130] train loss: 0.723
[18,   135] train loss: 0.725
[18,   140] train loss: 0.695
[18,   145] train loss: 0.744
[18,   150] train loss: 0.708
[18,   155] train loss: 0.725
[18,   160] train loss: 0.724
[18,   165] train loss: 0.707
[18,   170] train loss: 0.723
[18,   175] train loss: 0.713
[18,   180] train loss: 0.707
[18,   185] train loss: 0.728
[18,   190] train loss: 0.707
[18,   195] train loss: 0.722
[18,   200] train loss: 0.715
[18,   205] train loss: 0.715
[18,   210] train loss: 0.709
[18,   215] train loss: 0.704
[18,   220] train loss: 0.707
[18,   225] train loss: 0.720
[18,   230] train loss: 0.718
[18,   235] train loss: 0.697
[18,   240] train loss: 0.703
[18,   245] train loss: 0.725
[18,   250] train loss: 0.720
[18,   255] train loss: 0.694
[18,   260] train loss: 0.706
[18,   265] train loss: 0.712
[18,   270] train loss: 0.712
[18,   275] train loss: 0.715
[18,   280] train loss: 0.711
[18,   285] train loss: 0.720
[18,   290] train loss: 0.728
[18,   295] train loss: 0.716
[18,   300] train loss: 0.710
[18,   305] train loss: 0.720
[18,   310] train loss: 0.721
[18,   315] train loss: 0.718
[18,   320] train loss: 0.732
[18,   325] train loss: 0.732
[18,   330] train loss: 0.715
Finished Training
[18,     5] test loss: 0.705
[18,    10] test loss: 0.725
[18,    15] test loss: 0.711
[18,    20] test loss: 0.702
[18,    25] test loss: 0.706
[18,    30] test loss: 0.706
[18,    35] test loss: 0.675
[18,    40] test loss: 0.699
[18,    45] test loss: 0.701
[18,    50] test loss: 0.697
[18,    55] test loss: 0.718
[18,    60] test loss: 0.721
[18,    65] test loss: 0.687
[18,    70] test loss: 0.719
[18,    75] test loss: 0.684
[18,    80] test loss: 0.725
[18,    85] test loss: 0.698
[18,    90] test loss: 0.721
[18,    95] test loss: 0.710
[18,   100] test loss: 0.728
[18,   105] test loss: 0.720
[18,   110] test loss: 0.688
-----------------------------------------------------------------------------------------
Micro-Precision: 0.27082309126853943, Macro-Precision: nan

Micro-Recall: 0.2898321747779846, Macro-Recall: nan

Micro-F1: 0.2800053656101227, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  17 | time: 134.51s | valid loss  0.71 | valid ppl     2.04
-----------------------------------------------------------------------------------------
[19,     5] train loss: 0.707
[19,    10] train loss: 0.724
[19,    15] train loss: 0.703
[19,    20] train loss: 0.716
[19,    25] train loss: 0.716
[19,    30] train loss: 0.706
[19,    35] train loss: 0.708
[19,    40] train loss: 0.703
[19,    45] train loss: 0.716
[19,    50] train loss: 0.718
[19,    55] train loss: 0.718
[19,    60] train loss: 0.696
[19,    65] train loss: 0.707
[19,    70] train loss: 0.710
[19,    75] train loss: 0.718
[19,    80] train loss: 0.715
[19,    85] train loss: 0.721
[19,    90] train loss: 0.726
[19,    95] train loss: 0.711
[19,   100] train loss: 0.710
[19,   105] train loss: 0.716
[19,   110] train loss: 0.729
[19,   115] train loss: 0.714
[19,   120] train loss: 0.695
[19,   125] train loss: 0.694
[19,   130] train loss: 0.704
[19,   135] train loss: 0.721
[19,   140] train loss: 0.718
[19,   145] train loss: 0.705
[19,   150] train loss: 0.718
[19,   155] train loss: 0.705
[19,   160] train loss: 0.718
[19,   165] train loss: 0.717
[19,   170] train loss: 0.716
[19,   175] train loss: 0.705
[19,   180] train loss: 0.722
[19,   185] train loss: 0.723
[19,   190] train loss: 0.707
[19,   195] train loss: 0.694
[19,   200] train loss: 0.707
[19,   205] train loss: 0.713
[19,   210] train loss: 0.710
[19,   215] train loss: 0.702
[19,   220] train loss: 0.727
[19,   225] train loss: 0.715
[19,   230] train loss: 0.722
[19,   235] train loss: 0.729
[19,   240] train loss: 0.723
[19,   245] train loss: 0.707
[19,   250] train loss: 0.703
[19,   255] train loss: 0.695
[19,   260] train loss: 0.711
[19,   265] train loss: 0.721
[19,   270] train loss: 0.706
[19,   275] train loss: 0.720
[19,   280] train loss: 0.712
[19,   285] train loss: 0.706
[19,   290] train loss: 0.697
[19,   295] train loss: 0.719
[19,   300] train loss: 0.719
[19,   305] train loss: 0.707
[19,   310] train loss: 0.698
[19,   315] train loss: 0.705
[19,   320] train loss: 0.716
[19,   325] train loss: 0.721
[19,   330] train loss: 0.702
Finished Training
[19,     5] test loss: 0.703
[19,    10] test loss: 0.697
[19,    15] test loss: 0.707
[19,    20] test loss: 0.713
[19,    25] test loss: 0.696
[19,    30] test loss: 0.718
[19,    35] test loss: 0.707
[19,    40] test loss: 0.717
[19,    45] test loss: 0.705
[19,    50] test loss: 0.696
[19,    55] test loss: 0.702
[19,    60] test loss: 0.679
[19,    65] test loss: 0.695
[19,    70] test loss: 0.706
[19,    75] test loss: 0.703
[19,    80] test loss: 0.699
[19,    85] test loss: 0.709
[19,    90] test loss: 0.697
[19,    95] test loss: 0.704
[19,   100] test loss: 0.694
[19,   105] test loss: 0.701
[19,   110] test loss: 0.693
-----------------------------------------------------------------------------------------
Micro-Precision: 0.27664685249328613, Macro-Precision: nan

Micro-Recall: 0.3016548752784729, Macro-Recall: nan

Micro-F1: 0.28861016035079956, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  18 | time: 134.94s | valid loss  0.71 | valid ppl     2.03
-----------------------------------------------------------------------------------------
[20,     5] train loss: 0.705
[20,    10] train loss: 0.712
[20,    15] train loss: 0.710
[20,    20] train loss: 0.701
[20,    25] train loss: 0.723
[20,    30] train loss: 0.710
[20,    35] train loss: 0.698
[20,    40] train loss: 0.708
[20,    45] train loss: 0.703
[20,    50] train loss: 0.703
[20,    55] train loss: 0.687
[20,    60] train loss: 0.709
[20,    65] train loss: 0.721
[20,    70] train loss: 0.715
[20,    75] train loss: 0.693
[20,    80] train loss: 0.714
[20,    85] train loss: 0.705
[20,    90] train loss: 0.716
[20,    95] train loss: 0.721
[20,   100] train loss: 0.699
[20,   105] train loss: 0.700
[20,   110] train loss: 0.709
[20,   115] train loss: 0.726
[20,   120] train loss: 0.707
[20,   125] train loss: 0.688
[20,   130] train loss: 0.712
[20,   135] train loss: 0.709
[20,   140] train loss: 0.696
[20,   145] train loss: 0.677
[20,   150] train loss: 0.699
[20,   155] train loss: 0.714
[20,   160] train loss: 0.708
[20,   165] train loss: 0.705
[20,   170] train loss: 0.708
[20,   175] train loss: 0.701
[20,   180] train loss: 0.707
[20,   185] train loss: 0.706
[20,   190] train loss: 0.712
[20,   195] train loss: 0.715
[20,   200] train loss: 0.705
[20,   205] train loss: 0.711
[20,   210] train loss: 0.701
[20,   215] train loss: 0.713
[20,   220] train loss: 0.701
[20,   225] train loss: 0.723
[20,   230] train loss: 0.685
[20,   235] train loss: 0.707
[20,   240] train loss: 0.718
[20,   245] train loss: 0.733
[20,   250] train loss: 0.705
[20,   255] train loss: 0.690
[20,   260] train loss: 0.693
[20,   265] train loss: 0.694
[20,   270] train loss: 0.698
[20,   275] train loss: 0.701
[20,   280] train loss: 0.712
[20,   285] train loss: 0.706
[20,   290] train loss: 0.717
[20,   295] train loss: 0.703
[20,   300] train loss: 0.703
[20,   305] train loss: 0.712
[20,   310] train loss: 0.711
[20,   315] train loss: 0.709
[20,   320] train loss: 0.697
[20,   325] train loss: 0.711
[20,   330] train loss: 0.715
Finished Training
[20,     5] test loss: 0.687
[20,    10] test loss: 0.709
[20,    15] test loss: 0.686
[20,    20] test loss: 0.710
[20,    25] test loss: 0.686
[20,    30] test loss: 0.722
[20,    35] test loss: 0.682
[20,    40] test loss: 0.697
[20,    45] test loss: 0.733
[20,    50] test loss: 0.694
[20,    55] test loss: 0.688
[20,    60] test loss: 0.711
[20,    65] test loss: 0.674
[20,    70] test loss: 0.706
[20,    75] test loss: 0.709
[20,    80] test loss: 0.697
[20,    85] test loss: 0.693
[20,    90] test loss: 0.692
[20,    95] test loss: 0.700
[20,   100] test loss: 0.700
[20,   105] test loss: 0.699
[20,   110] test loss: 0.684
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2787696123123169, Macro-Precision: nan

Micro-Recall: 0.30181583762168884, Macro-Recall: nan

Micro-F1: 0.2898353338241577, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  19 | time: 134.68s | valid loss  0.70 | valid ppl     2.02
-----------------------------------------------------------------------------------------
[21,     5] train loss: 0.696
[21,    10] train loss: 0.714
[21,    15] train loss: 0.694
[21,    20] train loss: 0.698
[21,    25] train loss: 0.708
[21,    30] train loss: 0.711
[21,    35] train loss: 0.697
[21,    40] train loss: 0.692
[21,    45] train loss: 0.689
[21,    50] train loss: 0.711
[21,    55] train loss: 0.695
[21,    60] train loss: 0.689
[21,    65] train loss: 0.686
[21,    70] train loss: 0.716
[21,    75] train loss: 0.689
[21,    80] train loss: 0.701
[21,    85] train loss: 0.711
[21,    90] train loss: 0.710
[21,    95] train loss: 0.698
[21,   100] train loss: 0.690
[21,   105] train loss: 0.685
[21,   110] train loss: 0.703
[21,   115] train loss: 0.688
[21,   120] train loss: 0.696
[21,   125] train loss: 0.707
[21,   130] train loss: 0.707
[21,   135] train loss: 0.689
[21,   140] train loss: 0.707
[21,   145] train loss: 0.691
[21,   150] train loss: 0.700
[21,   155] train loss: 0.699
[21,   160] train loss: 0.697
[21,   165] train loss: 0.701
[21,   170] train loss: 0.702
[21,   175] train loss: 0.725
[21,   180] train loss: 0.702
[21,   185] train loss: 0.697
[21,   190] train loss: 0.716
[21,   195] train loss: 0.703
[21,   200] train loss: 0.709
[21,   205] train loss: 0.708
[21,   210] train loss: 0.700
[21,   215] train loss: 0.695
[21,   220] train loss: 0.704
[21,   225] train loss: 0.711
[21,   230] train loss: 0.708
[21,   235] train loss: 0.708
[21,   240] train loss: 0.698
[21,   245] train loss: 0.698
[21,   250] train loss: 0.712
[21,   255] train loss: 0.708
[21,   260] train loss: 0.710
[21,   265] train loss: 0.695
[21,   270] train loss: 0.695
[21,   275] train loss: 0.687
[21,   280] train loss: 0.694
[21,   285] train loss: 0.717
[21,   290] train loss: 0.719
[21,   295] train loss: 0.683
[21,   300] train loss: 0.717
[21,   305] train loss: 0.707
[21,   310] train loss: 0.702
[21,   315] train loss: 0.709
[21,   320] train loss: 0.693
[21,   325] train loss: 0.707
[21,   330] train loss: 0.692
Finished Training
[21,     5] test loss: 0.712
[21,    10] test loss: 0.701
[21,    15] test loss: 0.710
[21,    20] test loss: 0.675
[21,    25] test loss: 0.692
[21,    30] test loss: 0.689
[21,    35] test loss: 0.683
[21,    40] test loss: 0.682
[21,    45] test loss: 0.687
[21,    50] test loss: 0.677
[21,    55] test loss: 0.699
[21,    60] test loss: 0.684
[21,    65] test loss: 0.696
[21,    70] test loss: 0.689
[21,    75] test loss: 0.679
[21,    80] test loss: 0.696
[21,    85] test loss: 0.699
[21,    90] test loss: 0.694
[21,    95] test loss: 0.712
[21,   100] test loss: 0.703
[21,   105] test loss: 0.692
[21,   110] test loss: 0.711
-----------------------------------------------------------------------------------------
Micro-Precision: 0.28295981884002686, Macro-Precision: nan

Micro-Recall: 0.30427849292755127, Macro-Recall: nan

Micro-F1: 0.29323217272758484, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  20 | time: 135.04s | valid loss  0.70 | valid ppl     2.01
-----------------------------------------------------------------------------------------
[22,     5] train loss: 0.705
[22,    10] train loss: 0.700
[22,    15] train loss: 0.700
[22,    20] train loss: 0.683
[22,    25] train loss: 0.699
[22,    30] train loss: 0.696
[22,    35] train loss: 0.700
[22,    40] train loss: 0.697
[22,    45] train loss: 0.696
[22,    50] train loss: 0.699
[22,    55] train loss: 0.701
[22,    60] train loss: 0.697
[22,    65] train loss: 0.681
[22,    70] train loss: 0.692
[22,    75] train loss: 0.707
[22,    80] train loss: 0.688
[22,    85] train loss: 0.701
[22,    90] train loss: 0.714
[22,    95] train loss: 0.697
[22,   100] train loss: 0.691
[22,   105] train loss: 0.702
[22,   110] train loss: 0.680
[22,   115] train loss: 0.691
[22,   120] train loss: 0.690
[22,   125] train loss: 0.699
[22,   130] train loss: 0.705
[22,   135] train loss: 0.706
[22,   140] train loss: 0.692
[22,   145] train loss: 0.692
[22,   150] train loss: 0.695
[22,   155] train loss: 0.698
[22,   160] train loss: 0.709
[22,   165] train loss: 0.695
[22,   170] train loss: 0.689
[22,   175] train loss: 0.696
[22,   180] train loss: 0.690
[22,   185] train loss: 0.711
[22,   190] train loss: 0.698
[22,   195] train loss: 0.697
[22,   200] train loss: 0.703
[22,   205] train loss: 0.690
[22,   210] train loss: 0.697
[22,   215] train loss: 0.695
[22,   220] train loss: 0.702
[22,   225] train loss: 0.700
[22,   230] train loss: 0.691
[22,   235] train loss: 0.708
[22,   240] train loss: 0.690
[22,   245] train loss: 0.690
[22,   250] train loss: 0.698
[22,   255] train loss: 0.712
[22,   260] train loss: 0.694
[22,   265] train loss: 0.682
[22,   270] train loss: 0.717
[22,   275] train loss: 0.706
[22,   280] train loss: 0.691
[22,   285] train loss: 0.691
[22,   290] train loss: 0.704
[22,   295] train loss: 0.686
[22,   300] train loss: 0.703
[22,   305] train loss: 0.692
[22,   310] train loss: 0.691
[22,   315] train loss: 0.686
[22,   320] train loss: 0.691
[22,   325] train loss: 0.704
[22,   330] train loss: 0.691
Finished Training
[22,     5] test loss: 0.688
[22,    10] test loss: 0.705
[22,    15] test loss: 0.712
[22,    20] test loss: 0.685
[22,    25] test loss: 0.692
[22,    30] test loss: 0.692
[22,    35] test loss: 0.704
[22,    40] test loss: 0.676
[22,    45] test loss: 0.709
[22,    50] test loss: 0.700
[22,    55] test loss: 0.682
[22,    60] test loss: 0.694
[22,    65] test loss: 0.688
[22,    70] test loss: 0.667
[22,    75] test loss: 0.676
[22,    80] test loss: 0.700
[22,    85] test loss: 0.694
[22,    90] test loss: 0.683
[22,    95] test loss: 0.691
[22,   100] test loss: 0.674
[22,   105] test loss: 0.707
[22,   110] test loss: 0.676
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2857201099395752, Macro-Precision: nan

Micro-Recall: 0.3071419298648834, Macro-Recall: nan

Micro-F1: 0.2960439920425415, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  21 | time: 134.63s | valid loss  0.70 | valid ppl     2.01
-----------------------------------------------------------------------------------------
[23,     5] train loss: 0.689
[23,    10] train loss: 0.680
[23,    15] train loss: 0.697
[23,    20] train loss: 0.695
[23,    25] train loss: 0.685
[23,    30] train loss: 0.697
[23,    35] train loss: 0.688
[23,    40] train loss: 0.703
[23,    45] train loss: 0.692
[23,    50] train loss: 0.694
[23,    55] train loss: 0.692
[23,    60] train loss: 0.698
[23,    65] train loss: 0.693
[23,    70] train loss: 0.692
[23,    75] train loss: 0.693
[23,    80] train loss: 0.688
[23,    85] train loss: 0.685
[23,    90] train loss: 0.691
[23,    95] train loss: 0.689
[23,   100] train loss: 0.699
[23,   105] train loss: 0.696
[23,   110] train loss: 0.702
[23,   115] train loss: 0.699
[23,   120] train loss: 0.692
[23,   125] train loss: 0.698
[23,   130] train loss: 0.702
[23,   135] train loss: 0.689
[23,   140] train loss: 0.698
[23,   145] train loss: 0.694
[23,   150] train loss: 0.690
[23,   155] train loss: 0.675
[23,   160] train loss: 0.685
[23,   165] train loss: 0.696
[23,   170] train loss: 0.693
[23,   175] train loss: 0.681
[23,   180] train loss: 0.684
[23,   185] train loss: 0.695
[23,   190] train loss: 0.700
[23,   195] train loss: 0.706
[23,   200] train loss: 0.716
[23,   205] train loss: 0.686
[23,   210] train loss: 0.689
[23,   215] train loss: 0.690
[23,   220] train loss: 0.696
[23,   225] train loss: 0.700
[23,   230] train loss: 0.683
[23,   235] train loss: 0.682
[23,   240] train loss: 0.690
[23,   245] train loss: 0.674
[23,   250] train loss: 0.698
[23,   255] train loss: 0.694
[23,   260] train loss: 0.704
[23,   265] train loss: 0.681
[23,   270] train loss: 0.688
[23,   275] train loss: 0.690
[23,   280] train loss: 0.690
[23,   285] train loss: 0.704
[23,   290] train loss: 0.684
[23,   295] train loss: 0.686
[23,   300] train loss: 0.702
[23,   305] train loss: 0.694
[23,   310] train loss: 0.693
[23,   315] train loss: 0.685
[23,   320] train loss: 0.685
[23,   325] train loss: 0.686
[23,   330] train loss: 0.699
Finished Training
[23,     5] test loss: 0.698
[23,    10] test loss: 0.663
[23,    15] test loss: 0.688
[23,    20] test loss: 0.691
[23,    25] test loss: 0.689
[23,    30] test loss: 0.702
[23,    35] test loss: 0.704
[23,    40] test loss: 0.691
[23,    45] test loss: 0.683
[23,    50] test loss: 0.682
[23,    55] test loss: 0.689
[23,    60] test loss: 0.666
[23,    65] test loss: 0.692
[23,    70] test loss: 0.684
[23,    75] test loss: 0.697
[23,    80] test loss: 0.687
[23,    85] test loss: 0.669
[23,    90] test loss: 0.687
[23,    95] test loss: 0.685
[23,   100] test loss: 0.705
[23,   105] test loss: 0.682
[23,   110] test loss: 0.691
-----------------------------------------------------------------------------------------
Micro-Precision: 0.28796011209487915, Macro-Precision: nan

Micro-Recall: 0.30848807096481323, Macro-Recall: nan

Micro-F1: 0.29787084460258484, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  22 | time: 134.60s | valid loss  0.69 | valid ppl     2.00
-----------------------------------------------------------------------------------------
[24,     5] train loss: 0.696
[24,    10] train loss: 0.685
[24,    15] train loss: 0.692
[24,    20] train loss: 0.667
[24,    25] train loss: 0.687
[24,    30] train loss: 0.707
[24,    35] train loss: 0.692
[24,    40] train loss: 0.681
[24,    45] train loss: 0.676
[24,    50] train loss: 0.703
[24,    55] train loss: 0.699
[24,    60] train loss: 0.694
[24,    65] train loss: 0.679
[24,    70] train loss: 0.689
[24,    75] train loss: 0.686
[24,    80] train loss: 0.689
[24,    85] train loss: 0.677
[24,    90] train loss: 0.686
[24,    95] train loss: 0.695
[24,   100] train loss: 0.685
[24,   105] train loss: 0.671
[24,   110] train loss: 0.685
[24,   115] train loss: 0.690
[24,   120] train loss: 0.673
[24,   125] train loss: 0.676
[24,   130] train loss: 0.683
[24,   135] train loss: 0.693
[24,   140] train loss: 0.681
[24,   145] train loss: 0.682
[24,   150] train loss: 0.674
[24,   155] train loss: 0.685
[24,   160] train loss: 0.681
[24,   165] train loss: 0.691
[24,   170] train loss: 0.709
[24,   175] train loss: 0.692
[24,   180] train loss: 0.680
[24,   185] train loss: 0.686
[24,   190] train loss: 0.681
[24,   195] train loss: 0.685
[24,   200] train loss: 0.690
[24,   205] train loss: 0.692
[24,   210] train loss: 0.690
[24,   215] train loss: 0.681
[24,   220] train loss: 0.686
[24,   225] train loss: 0.689
[24,   230] train loss: 0.687
[24,   235] train loss: 0.689
[24,   240] train loss: 0.684
[24,   245] train loss: 0.681
[24,   250] train loss: 0.702
[24,   255] train loss: 0.690
[24,   260] train loss: 0.678
[24,   265] train loss: 0.688
[24,   270] train loss: 0.705
[24,   275] train loss: 0.687
[24,   280] train loss: 0.673
[24,   285] train loss: 0.691
[24,   290] train loss: 0.678
[24,   295] train loss: 0.698
[24,   300] train loss: 0.700
[24,   305] train loss: 0.698
[24,   310] train loss: 0.691
[24,   315] train loss: 0.706
[24,   320] train loss: 0.692
[24,   325] train loss: 0.704
[24,   330] train loss: 0.672
Finished Training
[24,     5] test loss: 0.657
[24,    10] test loss: 0.678
[24,    15] test loss: 0.693
[24,    20] test loss: 0.695
[24,    25] test loss: 0.690
[24,    30] test loss: 0.681
[24,    35] test loss: 0.676
[24,    40] test loss: 0.683
[24,    45] test loss: 0.693
[24,    50] test loss: 0.700
[24,    55] test loss: 0.670
[24,    60] test loss: 0.682
[24,    65] test loss: 0.684
[24,    70] test loss: 0.685
[24,    75] test loss: 0.688
[24,    80] test loss: 0.682
[24,    85] test loss: 0.682
[24,    90] test loss: 0.688
[24,    95] test loss: 0.680
[24,   100] test loss: 0.682
[24,   105] test loss: 0.699
[24,   110] test loss: 0.684
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2903531789779663, Macro-Precision: nan

Micro-Recall: 0.3090294599533081, Macro-Recall: nan

Micro-F1: 0.29940035939216614, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  23 | time: 134.41s | valid loss  0.69 | valid ppl     1.99
-----------------------------------------------------------------------------------------
[25,     5] train loss: 0.696
[25,    10] train loss: 0.681
[25,    15] train loss: 0.689
[25,    20] train loss: 0.680
[25,    25] train loss: 0.677
[25,    30] train loss: 0.665
[25,    35] train loss: 0.682
[25,    40] train loss: 0.688
[25,    45] train loss: 0.700
[25,    50] train loss: 0.695
[25,    55] train loss: 0.688
[25,    60] train loss: 0.697
[25,    65] train loss: 0.674
[25,    70] train loss: 0.678
[25,    75] train loss: 0.685
[25,    80] train loss: 0.684
[25,    85] train loss: 0.674
[25,    90] train loss: 0.671
[25,    95] train loss: 0.691
[25,   100] train loss: 0.674
[25,   105] train loss: 0.674
[25,   110] train loss: 0.697
[25,   115] train loss: 0.679
[25,   120] train loss: 0.691
[25,   125] train loss: 0.661
[25,   130] train loss: 0.678
[25,   135] train loss: 0.688
[25,   140] train loss: 0.673
[25,   145] train loss: 0.678
[25,   150] train loss: 0.678
[25,   155] train loss: 0.690
[25,   160] train loss: 0.701
[25,   165] train loss: 0.691
[25,   170] train loss: 0.683
[25,   175] train loss: 0.691
[25,   180] train loss: 0.672
[25,   185] train loss: 0.694
[25,   190] train loss: 0.683
[25,   195] train loss: 0.688
[25,   200] train loss: 0.680
[25,   205] train loss: 0.694
[25,   210] train loss: 0.700
[25,   215] train loss: 0.684
[25,   220] train loss: 0.673
[25,   225] train loss: 0.685
[25,   230] train loss: 0.684
[25,   235] train loss: 0.681
[25,   240] train loss: 0.688
[25,   245] train loss: 0.681
[25,   250] train loss: 0.673
[25,   255] train loss: 0.683
[25,   260] train loss: 0.676
[25,   265] train loss: 0.697
[25,   270] train loss: 0.689
[25,   275] train loss: 0.667
[25,   280] train loss: 0.685
[25,   285] train loss: 0.683
[25,   290] train loss: 0.682
[25,   295] train loss: 0.686
[25,   300] train loss: 0.671
[25,   305] train loss: 0.670
[25,   310] train loss: 0.692
[25,   315] train loss: 0.683
[25,   320] train loss: 0.689
[25,   325] train loss: 0.680
[25,   330] train loss: 0.672
Finished Training
[25,     5] test loss: 0.688
[25,    10] test loss: 0.662
[25,    15] test loss: 0.685
[25,    20] test loss: 0.669
[25,    25] test loss: 0.695
[25,    30] test loss: 0.682
[25,    35] test loss: 0.682
[25,    40] test loss: 0.697
[25,    45] test loss: 0.662
[25,    50] test loss: 0.681
[25,    55] test loss: 0.690
[25,    60] test loss: 0.675
[25,    65] test loss: 0.705
[25,    70] test loss: 0.680
[25,    75] test loss: 0.687
[25,    80] test loss: 0.674
[25,    85] test loss: 0.688
[25,    90] test loss: 0.688
[25,    95] test loss: 0.688
[25,   100] test loss: 0.673
[25,   105] test loss: 0.665
[25,   110] test loss: 0.681
-----------------------------------------------------------------------------------------
Micro-Precision: 0.29421308636665344, Macro-Precision: nan

Micro-Recall: 0.3078296184539795, Macro-Recall: nan

Micro-F1: 0.30086737871170044, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  24 | time: 133.85s | valid loss  0.69 | valid ppl     1.99
-----------------------------------------------------------------------------------------
