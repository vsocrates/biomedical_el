Restoring modules from user's fosscuda111, for system: "farnam-rhel7"
/home/vs428/project/MedMentions/full/pretraining5/entity_vocab.jsonl
Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
0
<torch.cuda.device object at 0x2b4971344b20>
1
GeForce GTX 1080 Ti
[1,     5] train loss: 1.008
[1,    10] train loss: 1.007
[1,    15] train loss: 1.008
[1,    20] train loss: 1.010
[1,    25] train loss: 1.008
[1,    30] train loss: 1.005
[1,    35] train loss: 1.004
[1,    40] train loss: 1.004
[1,    45] train loss: 0.999
[1,    50] train loss: 0.997
[1,    55] train loss: 0.998
[1,    60] train loss: 0.996
[1,    65] train loss: 0.996
[1,    70] train loss: 0.993
[1,    75] train loss: 0.991
[1,    80] train loss: 0.993
[1,    85] train loss: 0.991
[1,    90] train loss: 0.988
[1,    95] train loss: 0.987
[1,   100] train loss: 0.989
[1,   105] train loss: 0.984
[1,   110] train loss: 0.983
[1,   115] train loss: 0.982
[1,   120] train loss: 0.983
[1,   125] train loss: 0.981
[1,   130] train loss: 0.978
[1,   135] train loss: 0.977
[1,   140] train loss: 0.974
[1,   145] train loss: 0.975
[1,   150] train loss: 0.970
[1,   155] train loss: 0.967
[1,   160] train loss: 0.964
[1,   165] train loss: 0.963
Finished Training
[1,     5] test loss: 0.962
[1,    10] test loss: 0.959
[1,    15] test loss: 0.956
[1,    20] test loss: 0.960
[1,    25] test loss: 0.957
[1,    30] test loss: 0.955
[1,    35] test loss: 0.960
[1,    40] test loss: 0.961
[1,    45] test loss: 0.959
[1,    50] test loss: 0.961
[1,    55] test loss: 0.958
-----------------------------------------------------------------------------------------
Micro-Precision: 0.00682409293949604, Macro-Precision: nan

Micro-Recall: 0.007753412239253521, Macro-Recall: nan

Micro-F1: 0.0072591304779052734, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   0 | time: 111.22s | valid loss  0.98 | valid ppl     2.66
-----------------------------------------------------------------------------------------
[2,     5] train loss: 0.960
[2,    10] train loss: 0.957
[2,    15] train loss: 0.959
[2,    20] train loss: 0.954
[2,    25] train loss: 0.952
[2,    30] train loss: 0.948
[2,    35] train loss: 0.949
[2,    40] train loss: 0.946
[2,    45] train loss: 0.948
[2,    50] train loss: 0.951
[2,    55] train loss: 0.948
[2,    60] train loss: 0.945
[2,    65] train loss: 0.945
[2,    70] train loss: 0.940
[2,    75] train loss: 0.939
[2,    80] train loss: 0.940
[2,    85] train loss: 0.939
[2,    90] train loss: 0.938
[2,    95] train loss: 0.936
[2,   100] train loss: 0.935
[2,   105] train loss: 0.933
[2,   110] train loss: 0.930
[2,   115] train loss: 0.930
[2,   120] train loss: 0.931
[2,   125] train loss: 0.928
[2,   130] train loss: 0.925
[2,   135] train loss: 0.932
[2,   140] train loss: 0.926
[2,   145] train loss: 0.924
[2,   150] train loss: 0.924
[2,   155] train loss: 0.919
[2,   160] train loss: 0.926
[2,   165] train loss: 0.924
Finished Training
[2,     5] test loss: 0.907
[2,    10] test loss: 0.915
[2,    15] test loss: 0.908
[2,    20] test loss: 0.912
[2,    25] test loss: 0.906
[2,    30] test loss: 0.909
[2,    35] test loss: 0.914
[2,    40] test loss: 0.912
[2,    45] test loss: 0.907
[2,    50] test loss: 0.913
[2,    55] test loss: 0.915
-----------------------------------------------------------------------------------------
Micro-Precision: 0.02184607833623886, Macro-Precision: nan

Micro-Recall: 0.02462722547352314, Macro-Recall: nan

Micro-F1: 0.023153433576226234, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   1 | time: 123.49s | valid loss  0.93 | valid ppl     2.53
-----------------------------------------------------------------------------------------
[3,     5] train loss: 0.917
[3,    10] train loss: 0.915
[3,    15] train loss: 0.915
[3,    20] train loss: 0.912
[3,    25] train loss: 0.912
[3,    30] train loss: 0.908
[3,    35] train loss: 0.914
[3,    40] train loss: 0.901
[3,    45] train loss: 0.912
[3,    50] train loss: 0.904
[3,    55] train loss: 0.905
[3,    60] train loss: 0.901
[3,    65] train loss: 0.906
[3,    70] train loss: 0.905
[3,    75] train loss: 0.902
[3,    80] train loss: 0.903
[3,    85] train loss: 0.896
[3,    90] train loss: 0.905
[3,    95] train loss: 0.898
[3,   100] train loss: 0.900
[3,   105] train loss: 0.899
[3,   110] train loss: 0.899
[3,   115] train loss: 0.893
[3,   120] train loss: 0.897
[3,   125] train loss: 0.895
[3,   130] train loss: 0.888
[3,   135] train loss: 0.892
[3,   140] train loss: 0.891
[3,   145] train loss: 0.886
[3,   150] train loss: 0.894
[3,   155] train loss: 0.899
[3,   160] train loss: 0.894
[3,   165] train loss: 0.893
Finished Training
[3,     5] test loss: 0.878
[3,    10] test loss: 0.877
[3,    15] test loss: 0.877
[3,    20] test loss: 0.883
[3,    25] test loss: 0.878
[3,    30] test loss: 0.881
[3,    35] test loss: 0.879
[3,    40] test loss: 0.879
[3,    45] test loss: 0.877
[3,    50] test loss: 0.884
[3,    55] test loss: 0.878
-----------------------------------------------------------------------------------------
Micro-Precision: 0.053121890872716904, Macro-Precision: nan

Micro-Recall: 0.058574773371219635, Macro-Recall: nan

Micro-F1: 0.055715229362249374, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   2 | time: 124.48s | valid loss  0.90 | valid ppl     2.45
-----------------------------------------------------------------------------------------
[4,     5] train loss: 0.890
[4,    10] train loss: 0.886
[4,    15] train loss: 0.891
[4,    20] train loss: 0.883
[4,    25] train loss: 0.883
[4,    30] train loss: 0.885
[4,    35] train loss: 0.881
[4,    40] train loss: 0.890
[4,    45] train loss: 0.882
[4,    50] train loss: 0.875
[4,    55] train loss: 0.885
[4,    60] train loss: 0.882
[4,    65] train loss: 0.884
[4,    70] train loss: 0.879
[4,    75] train loss: 0.879
[4,    80] train loss: 0.877
[4,    85] train loss: 0.878
[4,    90] train loss: 0.885
[4,    95] train loss: 0.878
[4,   100] train loss: 0.873
[4,   105] train loss: 0.877
[4,   110] train loss: 0.877
[4,   115] train loss: 0.879
[4,   120] train loss: 0.870
[4,   125] train loss: 0.872
[4,   130] train loss: 0.866
[4,   135] train loss: 0.874
[4,   140] train loss: 0.873
[4,   145] train loss: 0.877
[4,   150] train loss: 0.876
[4,   155] train loss: 0.873
[4,   160] train loss: 0.867
[4,   165] train loss: 0.866
Finished Training
[4,     5] test loss: 0.858
[4,    10] test loss: 0.862
[4,    15] test loss: 0.861
[4,    20] test loss: 0.856
[4,    25] test loss: 0.858
[4,    30] test loss: 0.854
[4,    35] test loss: 0.855
[4,    40] test loss: 0.860
[4,    45] test loss: 0.862
[4,    50] test loss: 0.853
[4,    55] test loss: 0.857
-----------------------------------------------------------------------------------------
Micro-Precision: 0.07746820151805878, Macro-Precision: nan

Micro-Recall: 0.08413690328598022, Macro-Recall: nan

Micro-F1: 0.08066496253013611, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   3 | time: 124.54s | valid loss  0.87 | valid ppl     2.40
-----------------------------------------------------------------------------------------
[5,     5] train loss: 0.869
[5,    10] train loss: 0.870
[5,    15] train loss: 0.874
[5,    20] train loss: 0.861
[5,    25] train loss: 0.864
[5,    30] train loss: 0.868
[5,    35] train loss: 0.861
[5,    40] train loss: 0.857
[5,    45] train loss: 0.864
[5,    50] train loss: 0.868
[5,    55] train loss: 0.859
[5,    60] train loss: 0.866
[5,    65] train loss: 0.868
[5,    70] train loss: 0.852
[5,    75] train loss: 0.859
[5,    80] train loss: 0.868
[5,    85] train loss: 0.858
[5,    90] train loss: 0.860
[5,    95] train loss: 0.858
[5,   100] train loss: 0.854
[5,   105] train loss: 0.852
[5,   110] train loss: 0.863
[5,   115] train loss: 0.864
[5,   120] train loss: 0.859
[5,   125] train loss: 0.862
[5,   130] train loss: 0.863
[5,   135] train loss: 0.855
[5,   140] train loss: 0.854
[5,   145] train loss: 0.861
[5,   150] train loss: 0.848
[5,   155] train loss: 0.858
[5,   160] train loss: 0.854
[5,   165] train loss: 0.852
Finished Training
[5,     5] test loss: 0.841
[5,    10] test loss: 0.837
[5,    15] test loss: 0.840
[5,    20] test loss: 0.840
[5,    25] test loss: 0.839
[5,    30] test loss: 0.842
[5,    35] test loss: 0.845
[5,    40] test loss: 0.828
[5,    45] test loss: 0.841
[5,    50] test loss: 0.841
[5,    55] test loss: 0.835
-----------------------------------------------------------------------------------------
Micro-Precision: 0.10193979740142822, Macro-Precision: nan

Micro-Recall: 0.10758289694786072, Macro-Recall: nan

Micro-F1: 0.10468535125255585, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   4 | time: 123.97s | valid loss  0.85 | valid ppl     2.35
-----------------------------------------------------------------------------------------
[6,     5] train loss: 0.848
[6,    10] train loss: 0.849
[6,    15] train loss: 0.850
[6,    20] train loss: 0.850
[6,    25] train loss: 0.850
[6,    30] train loss: 0.853
[6,    35] train loss: 0.853
[6,    40] train loss: 0.847
[6,    45] train loss: 0.846
[6,    50] train loss: 0.854
[6,    55] train loss: 0.849
[6,    60] train loss: 0.859
[6,    65] train loss: 0.855
[6,    70] train loss: 0.840
[6,    75] train loss: 0.846
[6,    80] train loss: 0.840
[6,    85] train loss: 0.842
[6,    90] train loss: 0.844
[6,    95] train loss: 0.850
[6,   100] train loss: 0.845
[6,   105] train loss: 0.846
[6,   110] train loss: 0.835
[6,   115] train loss: 0.839
[6,   120] train loss: 0.839
[6,   125] train loss: 0.834
[6,   130] train loss: 0.835
[6,   135] train loss: 0.838
[6,   140] train loss: 0.840
[6,   145] train loss: 0.841
[6,   150] train loss: 0.838
[6,   155] train loss: 0.841
[6,   160] train loss: 0.844
[6,   165] train loss: 0.843
Finished Training
[6,     5] test loss: 0.814
[6,    10] test loss: 0.826
[6,    15] test loss: 0.815
[6,    20] test loss: 0.828
[6,    25] test loss: 0.822
[6,    30] test loss: 0.819
[6,    35] test loss: 0.817
[6,    40] test loss: 0.831
[6,    45] test loss: 0.830
[6,    50] test loss: 0.823
[6,    55] test loss: 0.831
-----------------------------------------------------------------------------------------
Micro-Precision: 0.12845315039157867, Macro-Precision: nan

Micro-Recall: 0.1361960470676422, Macro-Recall: nan

Micro-F1: 0.1322113275527954, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   5 | time: 124.51s | valid loss  0.84 | valid ppl     2.31
-----------------------------------------------------------------------------------------
[7,     5] train loss: 0.825
[7,    10] train loss: 0.833
[7,    15] train loss: 0.837
[7,    20] train loss: 0.838
[7,    25] train loss: 0.836
[7,    30] train loss: 0.836
[7,    35] train loss: 0.843
[7,    40] train loss: 0.830
[7,    45] train loss: 0.836
[7,    50] train loss: 0.831
[7,    55] train loss: 0.830
[7,    60] train loss: 0.827
[7,    65] train loss: 0.833
[7,    70] train loss: 0.838
[7,    75] train loss: 0.834
[7,    80] train loss: 0.834
[7,    85] train loss: 0.825
[7,    90] train loss: 0.823
[7,    95] train loss: 0.836
[7,   100] train loss: 0.830
[7,   105] train loss: 0.844
[7,   110] train loss: 0.822
[7,   115] train loss: 0.831
[7,   120] train loss: 0.838
[7,   125] train loss: 0.831
[7,   130] train loss: 0.826
[7,   135] train loss: 0.829
[7,   140] train loss: 0.829
[7,   145] train loss: 0.824
[7,   150] train loss: 0.823
[7,   155] train loss: 0.826
[7,   160] train loss: 0.834
[7,   165] train loss: 0.816
Finished Training
[7,     5] test loss: 0.807
[7,    10] test loss: 0.799
[7,    15] test loss: 0.805
[7,    20] test loss: 0.809
[7,    25] test loss: 0.813
[7,    30] test loss: 0.811
[7,    35] test loss: 0.812
[7,    40] test loss: 0.812
[7,    45] test loss: 0.807
[7,    50] test loss: 0.812
[7,    55] test loss: 0.814
-----------------------------------------------------------------------------------------
Micro-Precision: 0.14576485753059387, Macro-Precision: nan

Micro-Recall: 0.16221119463443756, Macro-Recall: nan

Micro-F1: 0.15354889631271362, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   6 | time: 125.56s | valid loss  0.82 | valid ppl     2.28
-----------------------------------------------------------------------------------------
[8,     5] train loss: 0.822
[8,    10] train loss: 0.828
[8,    15] train loss: 0.828
[8,    20] train loss: 0.824
[8,    25] train loss: 0.822
[8,    30] train loss: 0.825
[8,    35] train loss: 0.826
[8,    40] train loss: 0.818
[8,    45] train loss: 0.819
[8,    50] train loss: 0.818
[8,    55] train loss: 0.821
[8,    60] train loss: 0.823
[8,    65] train loss: 0.811
[8,    70] train loss: 0.818
[8,    75] train loss: 0.831
[8,    80] train loss: 0.830
[8,    85] train loss: 0.818
[8,    90] train loss: 0.817
[8,    95] train loss: 0.821
[8,   100] train loss: 0.818
[8,   105] train loss: 0.807
[8,   110] train loss: 0.814
[8,   115] train loss: 0.819
[8,   120] train loss: 0.809
[8,   125] train loss: 0.815
[8,   130] train loss: 0.818
[8,   135] train loss: 0.820
[8,   140] train loss: 0.818
[8,   145] train loss: 0.815
[8,   150] train loss: 0.814
[8,   155] train loss: 0.812
[8,   160] train loss: 0.821
[8,   165] train loss: 0.821
Finished Training
[8,     5] test loss: 0.804
[8,    10] test loss: 0.799
[8,    15] test loss: 0.799
[8,    20] test loss: 0.797
[8,    25] test loss: 0.787
[8,    30] test loss: 0.802
[8,    35] test loss: 0.798
[8,    40] test loss: 0.796
[8,    45] test loss: 0.798
[8,    50] test loss: 0.801
[8,    55] test loss: 0.795
-----------------------------------------------------------------------------------------
Micro-Precision: 0.16231124103069305, Macro-Precision: nan

Micro-Recall: 0.16748112440109253, Macro-Recall: nan

Micro-F1: 0.16485565900802612, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   7 | time: 124.07s | valid loss  0.81 | valid ppl     2.25
-----------------------------------------------------------------------------------------
[9,     5] train loss: 0.808
[9,    10] train loss: 0.822
[9,    15] train loss: 0.813
[9,    20] train loss: 0.806
[9,    25] train loss: 0.817
[9,    30] train loss: 0.808
[9,    35] train loss: 0.804
[9,    40] train loss: 0.812
[9,    45] train loss: 0.814
[9,    50] train loss: 0.803
[9,    55] train loss: 0.807
[9,    60] train loss: 0.811
[9,    65] train loss: 0.812
[9,    70] train loss: 0.819
[9,    75] train loss: 0.809
[9,    80] train loss: 0.801
[9,    85] train loss: 0.811
[9,    90] train loss: 0.817
[9,    95] train loss: 0.810
[9,   100] train loss: 0.810
[9,   105] train loss: 0.812
[9,   110] train loss: 0.807
[9,   115] train loss: 0.807
[9,   120] train loss: 0.812
[9,   125] train loss: 0.809
[9,   130] train loss: 0.808
[9,   135] train loss: 0.804
[9,   140] train loss: 0.805
[9,   145] train loss: 0.798
[9,   150] train loss: 0.811
[9,   155] train loss: 0.805
[9,   160] train loss: 0.805
[9,   165] train loss: 0.803
Finished Training
[9,     5] test loss: 0.788
[9,    10] test loss: 0.787
[9,    15] test loss: 0.786
[9,    20] test loss: 0.776
[9,    25] test loss: 0.787
[9,    30] test loss: 0.787
[9,    35] test loss: 0.796
[9,    40] test loss: 0.787
[9,    45] test loss: 0.797
[9,    50] test loss: 0.782
[9,    55] test loss: 0.789
-----------------------------------------------------------------------------------------
Micro-Precision: 0.1761472523212433, Macro-Precision: nan

Micro-Recall: 0.19226209819316864, Macro-Recall: nan

Micro-F1: 0.18385224044322968, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   8 | time: 125.13s | valid loss  0.80 | valid ppl     2.23
-----------------------------------------------------------------------------------------
[10,     5] train loss: 0.798
[10,    10] train loss: 0.798
[10,    15] train loss: 0.796
[10,    20] train loss: 0.804
[10,    25] train loss: 0.798
[10,    30] train loss: 0.798
[10,    35] train loss: 0.800
[10,    40] train loss: 0.793
[10,    45] train loss: 0.802
[10,    50] train loss: 0.805
[10,    55] train loss: 0.806
[10,    60] train loss: 0.799
[10,    65] train loss: 0.800
[10,    70] train loss: 0.805
[10,    75] train loss: 0.807
[10,    80] train loss: 0.791
[10,    85] train loss: 0.805
[10,    90] train loss: 0.795
[10,    95] train loss: 0.794
[10,   100] train loss: 0.789
[10,   105] train loss: 0.799
[10,   110] train loss: 0.802
[10,   115] train loss: 0.810
[10,   120] train loss: 0.801
[10,   125] train loss: 0.802
[10,   130] train loss: 0.800
[10,   135] train loss: 0.805
[10,   140] train loss: 0.808
[10,   145] train loss: 0.794
[10,   150] train loss: 0.808
[10,   155] train loss: 0.796
[10,   160] train loss: 0.802
[10,   165] train loss: 0.794
Finished Training
[10,     5] test loss: 0.780
[10,    10] test loss: 0.774
[10,    15] test loss: 0.777
[10,    20] test loss: 0.790
[10,    25] test loss: 0.775
[10,    30] test loss: 0.782
[10,    35] test loss: 0.778
[10,    40] test loss: 0.782
[10,    45] test loss: 0.774
[10,    50] test loss: 0.781
[10,    55] test loss: 0.774
-----------------------------------------------------------------------------------------
Micro-Precision: 0.18754370510578156, Macro-Precision: nan

Micro-Recall: 0.20404131710529327, Macro-Recall: nan

Micro-F1: 0.1954449862241745, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch   9 | time: 125.28s | valid loss  0.79 | valid ppl     2.21
-----------------------------------------------------------------------------------------
[11,     5] train loss: 0.787
[11,    10] train loss: 0.793
[11,    15] train loss: 0.805
[11,    20] train loss: 0.795
[11,    25] train loss: 0.785
[11,    30] train loss: 0.789
[11,    35] train loss: 0.795
[11,    40] train loss: 0.796
[11,    45] train loss: 0.797
[11,    50] train loss: 0.795
[11,    55] train loss: 0.797
[11,    60] train loss: 0.786
[11,    65] train loss: 0.788
[11,    70] train loss: 0.802
[11,    75] train loss: 0.793
[11,    80] train loss: 0.788
[11,    85] train loss: 0.798
[11,    90] train loss: 0.790
[11,    95] train loss: 0.792
[11,   100] train loss: 0.796
[11,   105] train loss: 0.793
[11,   110] train loss: 0.783
[11,   115] train loss: 0.795
[11,   120] train loss: 0.786
[11,   125] train loss: 0.789
[11,   130] train loss: 0.789
[11,   135] train loss: 0.789
[11,   140] train loss: 0.798
[11,   145] train loss: 0.794
[11,   150] train loss: 0.787
[11,   155] train loss: 0.790
[11,   160] train loss: 0.788
[11,   165] train loss: 0.782
Finished Training
[11,     5] test loss: 0.767
[11,    10] test loss: 0.781
[11,    15] test loss: 0.772
[11,    20] test loss: 0.762
[11,    25] test loss: 0.776
[11,    30] test loss: 0.768
[11,    35] test loss: 0.763
[11,    40] test loss: 0.758
[11,    45] test loss: 0.777
[11,    50] test loss: 0.779
[11,    55] test loss: 0.775
-----------------------------------------------------------------------------------------
Micro-Precision: 0.1989831030368805, Macro-Precision: nan

Micro-Recall: 0.2158658653497696, Macro-Recall: nan

Micro-F1: 0.20708094537258148, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  10 | time: 125.31s | valid loss  0.78 | valid ppl     2.19
-----------------------------------------------------------------------------------------
[12,     5] train loss: 0.785
[12,    10] train loss: 0.798
[12,    15] train loss: 0.788
[12,    20] train loss: 0.786
[12,    25] train loss: 0.799
[12,    30] train loss: 0.784
[12,    35] train loss: 0.781
[12,    40] train loss: 0.796
[12,    45] train loss: 0.778
[12,    50] train loss: 0.782
[12,    55] train loss: 0.787
[12,    60] train loss: 0.789
[12,    65] train loss: 0.780
[12,    70] train loss: 0.778
[12,    75] train loss: 0.788
[12,    80] train loss: 0.776
[12,    85] train loss: 0.783
[12,    90] train loss: 0.788
[12,    95] train loss: 0.787
[12,   100] train loss: 0.779
[12,   105] train loss: 0.795
[12,   110] train loss: 0.780
[12,   115] train loss: 0.785
[12,   120] train loss: 0.786
[12,   125] train loss: 0.776
[12,   130] train loss: 0.775
[12,   135] train loss: 0.778
[12,   140] train loss: 0.786
[12,   145] train loss: 0.779
[12,   150] train loss: 0.776
[12,   155] train loss: 0.782
[12,   160] train loss: 0.792
[12,   165] train loss: 0.781
Finished Training
[12,     5] test loss: 0.764
[12,    10] test loss: 0.765
[12,    15] test loss: 0.768
[12,    20] test loss: 0.756
[12,    25] test loss: 0.771
[12,    30] test loss: 0.752
[12,    35] test loss: 0.769
[12,    40] test loss: 0.761
[12,    45] test loss: 0.761
[12,    50] test loss: 0.756
[12,    55] test loss: 0.771
-----------------------------------------------------------------------------------------
Micro-Precision: 0.20860373973846436, Macro-Precision: nan

Micro-Recall: 0.22293105721473694, Macro-Recall: nan

Micro-F1: 0.21552956104278564, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  11 | time: 125.22s | valid loss  0.78 | valid ppl     2.18
-----------------------------------------------------------------------------------------
[13,     5] train loss: 0.770
[13,    10] train loss: 0.779
[13,    15] train loss: 0.772
[13,    20] train loss: 0.775
[13,    25] train loss: 0.780
[13,    30] train loss: 0.783
[13,    35] train loss: 0.768
[13,    40] train loss: 0.778
[13,    45] train loss: 0.782
[13,    50] train loss: 0.780
[13,    55] train loss: 0.786
[13,    60] train loss: 0.783
[13,    65] train loss: 0.774
[13,    70] train loss: 0.781
[13,    75] train loss: 0.776
[13,    80] train loss: 0.769
[13,    85] train loss: 0.778
[13,    90] train loss: 0.788
[13,    95] train loss: 0.766
[13,   100] train loss: 0.774
[13,   105] train loss: 0.775
[13,   110] train loss: 0.778
[13,   115] train loss: 0.776
[13,   120] train loss: 0.782
[13,   125] train loss: 0.777
[13,   130] train loss: 0.782
[13,   135] train loss: 0.787
[13,   140] train loss: 0.766
[13,   145] train loss: 0.780
[13,   150] train loss: 0.770
[13,   155] train loss: 0.774
[13,   160] train loss: 0.772
[13,   165] train loss: 0.787
Finished Training
[13,     5] test loss: 0.762
[13,    10] test loss: 0.764
[13,    15] test loss: 0.771
[13,    20] test loss: 0.761
[13,    25] test loss: 0.756
[13,    30] test loss: 0.757
[13,    35] test loss: 0.745
[13,    40] test loss: 0.755
[13,    45] test loss: 0.749
[13,    50] test loss: 0.753
[13,    55] test loss: 0.753
-----------------------------------------------------------------------------------------
Micro-Precision: 0.21697430312633514, Macro-Precision: nan

Micro-Recall: 0.23665651679039001, Macro-Recall: nan

Micro-F1: 0.22638842463493347, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  12 | time: 125.64s | valid loss  0.77 | valid ppl     2.16
-----------------------------------------------------------------------------------------
[14,     5] train loss: 0.766
[14,    10] train loss: 0.777
[14,    15] train loss: 0.770
[14,    20] train loss: 0.771
[14,    25] train loss: 0.769
[14,    30] train loss: 0.775
[14,    35] train loss: 0.767
[14,    40] train loss: 0.768
[14,    45] train loss: 0.769
[14,    50] train loss: 0.768
[14,    55] train loss: 0.766
[14,    60] train loss: 0.770
[14,    65] train loss: 0.774
[14,    70] train loss: 0.766
[14,    75] train loss: 0.773
[14,    80] train loss: 0.775
[14,    85] train loss: 0.773
[14,    90] train loss: 0.765
[14,    95] train loss: 0.777
[14,   100] train loss: 0.767
[14,   105] train loss: 0.775
[14,   110] train loss: 0.775
[14,   115] train loss: 0.773
[14,   120] train loss: 0.768
[14,   125] train loss: 0.774
[14,   130] train loss: 0.777
[14,   135] train loss: 0.763
[14,   140] train loss: 0.769
[14,   145] train loss: 0.771
[14,   150] train loss: 0.772
[14,   155] train loss: 0.762
[14,   160] train loss: 0.770
[14,   165] train loss: 0.770
Finished Training
[14,     5] test loss: 0.741
[14,    10] test loss: 0.759
[14,    15] test loss: 0.751
[14,    20] test loss: 0.749
[14,    25] test loss: 0.758
[14,    30] test loss: 0.752
[14,    35] test loss: 0.746
[14,    40] test loss: 0.749
[14,    45] test loss: 0.744
[14,    50] test loss: 0.752
[14,    55] test loss: 0.755
-----------------------------------------------------------------------------------------
Micro-Precision: 0.22379569709300995, Macro-Precision: nan

Micro-Recall: 0.2434745728969574, Macro-Recall: nan

Micro-F1: 0.23322074115276337, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  13 | time: 125.65s | valid loss  0.76 | valid ppl     2.15
-----------------------------------------------------------------------------------------
[15,     5] train loss: 0.752
[15,    10] train loss: 0.764
[15,    15] train loss: 0.770
[15,    20] train loss: 0.777
[15,    25] train loss: 0.762
[15,    30] train loss: 0.766
[15,    35] train loss: 0.764
[15,    40] train loss: 0.764
[15,    45] train loss: 0.752
[15,    50] train loss: 0.765
[15,    55] train loss: 0.764
[15,    60] train loss: 0.758
[15,    65] train loss: 0.769
[15,    70] train loss: 0.761
[15,    75] train loss: 0.763
[15,    80] train loss: 0.769
[15,    85] train loss: 0.774
[15,    90] train loss: 0.763
[15,    95] train loss: 0.760
[15,   100] train loss: 0.767
[15,   105] train loss: 0.768
[15,   110] train loss: 0.765
[15,   115] train loss: 0.763
[15,   120] train loss: 0.769
[15,   125] train loss: 0.768
[15,   130] train loss: 0.761
[15,   135] train loss: 0.762
[15,   140] train loss: 0.762
[15,   145] train loss: 0.767
[15,   150] train loss: 0.773
[15,   155] train loss: 0.749
[15,   160] train loss: 0.769
[15,   165] train loss: 0.761
Finished Training
[15,     5] test loss: 0.752
[15,    10] test loss: 0.742
[15,    15] test loss: 0.754
[15,    20] test loss: 0.739
[15,    25] test loss: 0.742
[15,    30] test loss: 0.739
[15,    35] test loss: 0.744
[15,    40] test loss: 0.742
[15,    45] test loss: 0.747
[15,    50] test loss: 0.753
[15,    55] test loss: 0.741
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2302767038345337, Macro-Precision: nan

Micro-Recall: 0.25048285722732544, Macro-Recall: nan

Micro-F1: 0.2399551421403885, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  14 | time: 125.56s | valid loss  0.76 | valid ppl     2.14
-----------------------------------------------------------------------------------------
[16,     5] train loss: 0.765
[16,    10] train loss: 0.765
[16,    15] train loss: 0.764
[16,    20] train loss: 0.758
[16,    25] train loss: 0.754
[16,    30] train loss: 0.770
[16,    35] train loss: 0.748
[16,    40] train loss: 0.763
[16,    45] train loss: 0.766
[16,    50] train loss: 0.758
[16,    55] train loss: 0.752
[16,    60] train loss: 0.752
[16,    65] train loss: 0.762
[16,    70] train loss: 0.751
[16,    75] train loss: 0.758
[16,    80] train loss: 0.758
[16,    85] train loss: 0.754
[16,    90] train loss: 0.756
[16,    95] train loss: 0.758
[16,   100] train loss: 0.761
[16,   105] train loss: 0.764
[16,   110] train loss: 0.749
[16,   115] train loss: 0.757
[16,   120] train loss: 0.763
[16,   125] train loss: 0.774
[16,   130] train loss: 0.756
[16,   135] train loss: 0.763
[16,   140] train loss: 0.745
[16,   145] train loss: 0.759
[16,   150] train loss: 0.758
[16,   155] train loss: 0.759
[16,   160] train loss: 0.763
[16,   165] train loss: 0.749
Finished Training
[16,     5] test loss: 0.746
[16,    10] test loss: 0.746
[16,    15] test loss: 0.747
[16,    20] test loss: 0.749
[16,    25] test loss: 0.735
[16,    30] test loss: 0.730
[16,    35] test loss: 0.732
[16,    40] test loss: 0.737
[16,    45] test loss: 0.747
[16,    50] test loss: 0.740
[16,    55] test loss: 0.730
-----------------------------------------------------------------------------------------
Micro-Precision: 0.23622465133666992, Macro-Precision: nan

Micro-Recall: 0.24776145815849304, Macro-Recall: nan

Micro-F1: 0.24185554683208466, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  15 | time: 124.90s | valid loss  0.75 | valid ppl     2.12
-----------------------------------------------------------------------------------------
[17,     5] train loss: 0.764
[17,    10] train loss: 0.759
[17,    15] train loss: 0.757
[17,    20] train loss: 0.754
[17,    25] train loss: 0.755
[17,    30] train loss: 0.743
[17,    35] train loss: 0.762
[17,    40] train loss: 0.755
[17,    45] train loss: 0.741
[17,    50] train loss: 0.758
[17,    55] train loss: 0.750
[17,    60] train loss: 0.756
[17,    65] train loss: 0.758
[17,    70] train loss: 0.757
[17,    75] train loss: 0.764
[17,    80] train loss: 0.752
[17,    85] train loss: 0.750
[17,    90] train loss: 0.746
[17,    95] train loss: 0.740
[17,   100] train loss: 0.761
[17,   105] train loss: 0.745
[17,   110] train loss: 0.745
[17,   115] train loss: 0.751
[17,   120] train loss: 0.751
[17,   125] train loss: 0.760
[17,   130] train loss: 0.757
[17,   135] train loss: 0.746
[17,   140] train loss: 0.757
[17,   145] train loss: 0.755
[17,   150] train loss: 0.747
[17,   155] train loss: 0.754
[17,   160] train loss: 0.745
[17,   165] train loss: 0.748
Finished Training
[17,     5] test loss: 0.723
[17,    10] test loss: 0.762
[17,    15] test loss: 0.728
[17,    20] test loss: 0.735
[17,    25] test loss: 0.737
[17,    30] test loss: 0.719
[17,    35] test loss: 0.738
[17,    40] test loss: 0.733
[17,    45] test loss: 0.726
[17,    50] test loss: 0.737
[17,    55] test loss: 0.737
-----------------------------------------------------------------------------------------
Micro-Precision: 0.24379445612430573, Macro-Precision: nan

Micro-Recall: 0.25779831409454346, Macro-Recall: nan

Micro-F1: 0.2506009042263031, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  16 | time: 125.06s | valid loss  0.75 | valid ppl     2.11
-----------------------------------------------------------------------------------------
[18,     5] train loss: 0.745
[18,    10] train loss: 0.743
[18,    15] train loss: 0.744
[18,    20] train loss: 0.742
[18,    25] train loss: 0.759
[18,    30] train loss: 0.751
[18,    35] train loss: 0.746
[18,    40] train loss: 0.737
[18,    45] train loss: 0.754
[18,    50] train loss: 0.747
[18,    55] train loss: 0.746
[18,    60] train loss: 0.752
[18,    65] train loss: 0.751
[18,    70] train loss: 0.752
[18,    75] train loss: 0.745
[18,    80] train loss: 0.750
[18,    85] train loss: 0.741
[18,    90] train loss: 0.757
[18,    95] train loss: 0.743
[18,   100] train loss: 0.741
[18,   105] train loss: 0.742
[18,   110] train loss: 0.759
[18,   115] train loss: 0.751
[18,   120] train loss: 0.746
[18,   125] train loss: 0.751
[18,   130] train loss: 0.762
[18,   135] train loss: 0.738
[18,   140] train loss: 0.734
[18,   145] train loss: 0.745
[18,   150] train loss: 0.737
[18,   155] train loss: 0.756
[18,   160] train loss: 0.746
[18,   165] train loss: 0.764
Finished Training
[18,     5] test loss: 0.726
[18,    10] test loss: 0.724
[18,    15] test loss: 0.728
[18,    20] test loss: 0.731
[18,    25] test loss: 0.731
[18,    30] test loss: 0.735
[18,    35] test loss: 0.724
[18,    40] test loss: 0.735
[18,    45] test loss: 0.735
[18,    50] test loss: 0.740
[18,    55] test loss: 0.732
-----------------------------------------------------------------------------------------
Micro-Precision: 0.24688959121704102, Macro-Precision: nan

Micro-Recall: 0.25841331481933594, Macro-Recall: nan

Micro-F1: 0.25252005457878113, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  17 | time: 124.96s | valid loss  0.74 | valid ppl     2.11
-----------------------------------------------------------------------------------------
[19,     5] train loss: 0.747
[19,    10] train loss: 0.738
[19,    15] train loss: 0.744
[19,    20] train loss: 0.741
[19,    25] train loss: 0.755
[19,    30] train loss: 0.749
[19,    35] train loss: 0.742
[19,    40] train loss: 0.748
[19,    45] train loss: 0.727
[19,    50] train loss: 0.734
[19,    55] train loss: 0.739
[19,    60] train loss: 0.742
[19,    65] train loss: 0.743
[19,    70] train loss: 0.750
[19,    75] train loss: 0.748
[19,    80] train loss: 0.743
[19,    85] train loss: 0.742
[19,    90] train loss: 0.748
[19,    95] train loss: 0.743
[19,   100] train loss: 0.741
[19,   105] train loss: 0.741
[19,   110] train loss: 0.739
[19,   115] train loss: 0.743
[19,   120] train loss: 0.756
[19,   125] train loss: 0.745
[19,   130] train loss: 0.741
[19,   135] train loss: 0.734
[19,   140] train loss: 0.742
[19,   145] train loss: 0.740
[19,   150] train loss: 0.731
[19,   155] train loss: 0.748
[19,   160] train loss: 0.736
[19,   165] train loss: 0.740
Finished Training
[19,     5] test loss: 0.717
[19,    10] test loss: 0.716
[19,    15] test loss: 0.723
[19,    20] test loss: 0.724
[19,    25] test loss: 0.729
[19,    30] test loss: 0.731
[19,    35] test loss: 0.725
[19,    40] test loss: 0.729
[19,    45] test loss: 0.729
[19,    50] test loss: 0.719
[19,    55] test loss: 0.740
-----------------------------------------------------------------------------------------
Micro-Precision: 0.25314778089523315, Macro-Precision: nan

Micro-Recall: 0.265641450881958, Macro-Recall: nan

Micro-F1: 0.2592441737651825, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  18 | time: 125.10s | valid loss  0.74 | valid ppl     2.09
-----------------------------------------------------------------------------------------
[20,     5] train loss: 0.749
[20,    10] train loss: 0.740
[20,    15] train loss: 0.742
[20,    20] train loss: 0.730
[20,    25] train loss: 0.730
[20,    30] train loss: 0.745
[20,    35] train loss: 0.745
[20,    40] train loss: 0.746
[20,    45] train loss: 0.743
[20,    50] train loss: 0.733
[20,    55] train loss: 0.740
[20,    60] train loss: 0.732
[20,    65] train loss: 0.742
[20,    70] train loss: 0.731
[20,    75] train loss: 0.734
[20,    80] train loss: 0.727
[20,    85] train loss: 0.737
[20,    90] train loss: 0.731
[20,    95] train loss: 0.740
[20,   100] train loss: 0.736
[20,   105] train loss: 0.735
[20,   110] train loss: 0.745
[20,   115] train loss: 0.744
[20,   120] train loss: 0.732
[20,   125] train loss: 0.732
[20,   130] train loss: 0.742
[20,   135] train loss: 0.733
[20,   140] train loss: 0.730
[20,   145] train loss: 0.740
[20,   150] train loss: 0.738
[20,   155] train loss: 0.733
[20,   160] train loss: 0.743
[20,   165] train loss: 0.730
Finished Training
[20,     5] test loss: 0.721
[20,    10] test loss: 0.727
[20,    15] test loss: 0.723
[20,    20] test loss: 0.727
[20,    25] test loss: 0.720
[20,    30] test loss: 0.728
[20,    35] test loss: 0.716
[20,    40] test loss: 0.716
[20,    45] test loss: 0.721
[20,    50] test loss: 0.719
[20,    55] test loss: 0.718
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2570360004901886, Macro-Precision: nan

Micro-Recall: 0.2704699635505676, Macro-Recall: nan

Micro-F1: 0.26358193159103394, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  19 | time: 125.08s | valid loss  0.73 | valid ppl     2.09
-----------------------------------------------------------------------------------------
[21,     5] train loss: 0.747
[21,    10] train loss: 0.737
[21,    15] train loss: 0.743
[21,    20] train loss: 0.740
[21,    25] train loss: 0.719
[21,    30] train loss: 0.738
[21,    35] train loss: 0.737
[21,    40] train loss: 0.740
[21,    45] train loss: 0.729
[21,    50] train loss: 0.735
[21,    55] train loss: 0.734
[21,    60] train loss: 0.749
[21,    65] train loss: 0.729
[21,    70] train loss: 0.724
[21,    75] train loss: 0.735
[21,    80] train loss: 0.727
[21,    85] train loss: 0.732
[21,    90] train loss: 0.737
[21,    95] train loss: 0.746
[21,   100] train loss: 0.735
[21,   105] train loss: 0.734
[21,   110] train loss: 0.740
[21,   115] train loss: 0.728
[21,   120] train loss: 0.719
[21,   125] train loss: 0.717
[21,   130] train loss: 0.728
[21,   135] train loss: 0.732
[21,   140] train loss: 0.731
[21,   145] train loss: 0.733
[21,   150] train loss: 0.718
[21,   155] train loss: 0.734
[21,   160] train loss: 0.725
[21,   165] train loss: 0.739
Finished Training
[21,     5] test loss: 0.727
[21,    10] test loss: 0.719
[21,    15] test loss: 0.715
[21,    20] test loss: 0.719
[21,    25] test loss: 0.711
[21,    30] test loss: 0.725
[21,    35] test loss: 0.720
[21,    40] test loss: 0.715
[21,    45] test loss: 0.707
[21,    50] test loss: 0.721
[21,    55] test loss: 0.716
-----------------------------------------------------------------------------------------
Micro-Precision: 0.26183202862739563, Macro-Precision: nan

Micro-Recall: 0.2734242379665375, Macro-Recall: nan

Micro-F1: 0.2675026059150696, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  20 | time: 125.20s | valid loss  0.73 | valid ppl     2.08
-----------------------------------------------------------------------------------------
[22,     5] train loss: 0.732
[22,    10] train loss: 0.733
[22,    15] train loss: 0.728
[22,    20] train loss: 0.730
[22,    25] train loss: 0.721
[22,    30] train loss: 0.733
[22,    35] train loss: 0.725
[22,    40] train loss: 0.730
[22,    45] train loss: 0.729
[22,    50] train loss: 0.720
[22,    55] train loss: 0.720
[22,    60] train loss: 0.732
[22,    65] train loss: 0.724
[22,    70] train loss: 0.721
[22,    75] train loss: 0.729
[22,    80] train loss: 0.723
[22,    85] train loss: 0.722
[22,    90] train loss: 0.745
[22,    95] train loss: 0.730
[22,   100] train loss: 0.738
[22,   105] train loss: 0.736
[22,   110] train loss: 0.729
[22,   115] train loss: 0.728
[22,   120] train loss: 0.735
[22,   125] train loss: 0.728
[22,   130] train loss: 0.717
[22,   135] train loss: 0.726
[22,   140] train loss: 0.739
[22,   145] train loss: 0.724
[22,   150] train loss: 0.730
[22,   155] train loss: 0.724
[22,   160] train loss: 0.727
[22,   165] train loss: 0.730
Finished Training
[22,     5] test loss: 0.728
[22,    10] test loss: 0.714
[22,    15] test loss: 0.695
[22,    20] test loss: 0.704
[22,    25] test loss: 0.719
[22,    30] test loss: 0.721
[22,    35] test loss: 0.727
[22,    40] test loss: 0.702
[22,    45] test loss: 0.724
[22,    50] test loss: 0.707
[22,    55] test loss: 0.722
-----------------------------------------------------------------------------------------
Micro-Precision: 0.26317286491394043, Macro-Precision: nan

Micro-Recall: 0.28428661823272705, Macro-Recall: nan

Micro-F1: 0.27332258224487305, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  21 | time: 125.56s | valid loss  0.73 | valid ppl     2.07
-----------------------------------------------------------------------------------------
[23,     5] train loss: 0.735
[23,    10] train loss: 0.724
[23,    15] train loss: 0.733
[23,    20] train loss: 0.721
[23,    25] train loss: 0.718
[23,    30] train loss: 0.717
[23,    35] train loss: 0.729
[23,    40] train loss: 0.719
[23,    45] train loss: 0.729
[23,    50] train loss: 0.733
[23,    55] train loss: 0.704
[23,    60] train loss: 0.724
[23,    65] train loss: 0.734
[23,    70] train loss: 0.734
[23,    75] train loss: 0.730
[23,    80] train loss: 0.726
[23,    85] train loss: 0.731
[23,    90] train loss: 0.726
[23,    95] train loss: 0.712
[23,   100] train loss: 0.717
[23,   105] train loss: 0.715
[23,   110] train loss: 0.724
[23,   115] train loss: 0.725
[23,   120] train loss: 0.724
[23,   125] train loss: 0.724
[23,   130] train loss: 0.722
[23,   135] train loss: 0.723
[23,   140] train loss: 0.733
[23,   145] train loss: 0.717
[23,   150] train loss: 0.726
[23,   155] train loss: 0.725
[23,   160] train loss: 0.719
[23,   165] train loss: 0.723
Finished Training
[23,     5] test loss: 0.717
[23,    10] test loss: 0.708
[23,    15] test loss: 0.717
[23,    20] test loss: 0.721
[23,    25] test loss: 0.698
[23,    30] test loss: 0.718
[23,    35] test loss: 0.710
[23,    40] test loss: 0.707
[23,    45] test loss: 0.696
[23,    50] test loss: 0.706
[23,    55] test loss: 0.718
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2695118188858032, Macro-Precision: nan

Micro-Recall: 0.2903296649456024, Macro-Recall: nan

Micro-F1: 0.2795336842536926, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  22 | time: 125.71s | valid loss  0.72 | valid ppl     2.06
-----------------------------------------------------------------------------------------
[24,     5] train loss: 0.727
[24,    10] train loss: 0.713
[24,    15] train loss: 0.721
[24,    20] train loss: 0.716
[24,    25] train loss: 0.729
[24,    30] train loss: 0.711
[24,    35] train loss: 0.721
[24,    40] train loss: 0.720
[24,    45] train loss: 0.728
[24,    50] train loss: 0.713
[24,    55] train loss: 0.720
[24,    60] train loss: 0.725
[24,    65] train loss: 0.710
[24,    70] train loss: 0.716
[24,    75] train loss: 0.712
[24,    80] train loss: 0.727
[24,    85] train loss: 0.715
[24,    90] train loss: 0.725
[24,    95] train loss: 0.715
[24,   100] train loss: 0.727
[24,   105] train loss: 0.716
[24,   110] train loss: 0.719
[24,   115] train loss: 0.721
[24,   120] train loss: 0.714
[24,   125] train loss: 0.722
[24,   130] train loss: 0.731
[24,   135] train loss: 0.724
[24,   140] train loss: 0.720
[24,   145] train loss: 0.721
[24,   150] train loss: 0.718
[24,   155] train loss: 0.733
[24,   160] train loss: 0.710
[24,   165] train loss: 0.714
Finished Training
[24,     5] test loss: 0.701
[24,    10] test loss: 0.716
[24,    15] test loss: 0.708
[24,    20] test loss: 0.719
[24,    25] test loss: 0.711
[24,    30] test loss: 0.710
[24,    35] test loss: 0.705
[24,    40] test loss: 0.706
[24,    45] test loss: 0.703
[24,    50] test loss: 0.705
[24,    55] test loss: 0.700
-----------------------------------------------------------------------------------------
Micro-Precision: 0.2717675268650055, Macro-Precision: nan

Micro-Recall: 0.29379746317863464, Macro-Recall: nan

Micro-F1: 0.2823534309864044, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  23 | time: 125.55s | valid loss  0.72 | valid ppl     2.06
-----------------------------------------------------------------------------------------
[25,     5] train loss: 0.713
[25,    10] train loss: 0.714
[25,    15] train loss: 0.712
[25,    20] train loss: 0.717
[25,    25] train loss: 0.711
[25,    30] train loss: 0.713
[25,    35] train loss: 0.716
[25,    40] train loss: 0.719
[25,    45] train loss: 0.721
[25,    50] train loss: 0.719
[25,    55] train loss: 0.722
[25,    60] train loss: 0.717
[25,    65] train loss: 0.718
[25,    70] train loss: 0.728
[25,    75] train loss: 0.713
[25,    80] train loss: 0.717
[25,    85] train loss: 0.712
[25,    90] train loss: 0.715
[25,    95] train loss: 0.710
[25,   100] train loss: 0.718
[25,   105] train loss: 0.711
[25,   110] train loss: 0.715
[25,   115] train loss: 0.709
[25,   120] train loss: 0.717
[25,   125] train loss: 0.720
[25,   130] train loss: 0.723
[25,   135] train loss: 0.712
[25,   140] train loss: 0.714
[25,   145] train loss: 0.712
[25,   150] train loss: 0.717
[25,   155] train loss: 0.721
[25,   160] train loss: 0.724
[25,   165] train loss: 0.709
Finished Training
[25,     5] test loss: 0.702
[25,    10] test loss: 0.718
[25,    15] test loss: 0.707
[25,    20] test loss: 0.715
[25,    25] test loss: 0.702
[25,    30] test loss: 0.692
[25,    35] test loss: 0.713
[25,    40] test loss: 0.704
[25,    45] test loss: 0.690
[25,    50] test loss: 0.698
[25,    55] test loss: 0.703
-----------------------------------------------------------------------------------------
Micro-Precision: 0.27487683296203613, Macro-Precision: nan

Micro-Recall: 0.28813260793685913, Macro-Recall: nan

Micro-F1: 0.28134867548942566, Macro-F1: nan

-----------------------------------------------------------------------------------------
-----------------------------------------------------------------------------------------
| end of epoch  24 | time: 124.87s | valid loss  0.72 | valid ppl     2.05
-----------------------------------------------------------------------------------------
