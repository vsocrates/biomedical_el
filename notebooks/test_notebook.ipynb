{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49e0bf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8aa563e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requi9red module\n",
    "import sys\n",
    "  \n",
    "# append the path of the\n",
    "# parent directory\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "atomic-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss, NLLLoss\n",
    "import torch.nn.functional as F \n",
    "from torch import autograd\n",
    "\n",
    "import numpy as np \n",
    "\n",
    "from transformers.models.luke.configuration_luke import LukeConfig\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "\n",
    "from entitylinker.lukemodel import LukeModel \n",
    "from entitylinker.model_utils import *\n",
    "\n",
    "from entitylinker.dataset import MedMentionsDataset, Collater\n",
    "from entitylinker.model import EntityLinker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "298a9e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "<torch.cuda.device object at 0x2b5a2572cee0>\n",
      "1\n",
      "Quadro RTX 5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "    print(torch.cuda.current_device())\n",
    "    print(torch.cuda.device(0))\n",
    "    print(torch.cuda.device_count())\n",
    "    print(torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "hollow-spouse",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_data_file = \"/home/vs428/project/MedMentions/full/data/corpus_pubtator.txt\"\n",
    "train_pmids_file = \"/home/vs428/project/MedMentions/full/data/corpus_pubtator_pmids_trng.txt\"\n",
    "test_pmids_file = \"/home/vs428/project/MedMentions/full/data/corpus_pubtator_pmids_test.txt\"\n",
    "\n",
    "entity_vocab_file = \"/home/vs428/project/MedMentions/full/pretraining3/entity_vocab.jsonl\"\n",
    "entity_embedding_model = \"/home/vs428/project/MedMentions/full/pretraining3//model_epoch20.bin\"\n",
    "entity_embedding_metadata = \"/home/vs428/project/MedMentions/full/pretraining3//metadata.json\"\n",
    "\n",
    "MAX_LENGTH = 128\n",
    "MODEL_NAME = 'microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "105b346d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Tokenizer and DataLoader\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "collater = Collater(tokenizer, max_length = MAX_LENGTH)\n",
    "train_dataset = MedMentionsDataset(input_data_file, train_pmids_file, tokenizer, entity_vocab_file, \n",
    "                             max_length = MAX_LENGTH, stride=0, first_token_ent_only=True)\n",
    "test_dataset = MedMentionsDataset(input_data_file, test_pmids_file, tokenizer, entity_vocab_file, \n",
    "                             max_length = MAX_LENGTH, stride=0, first_token_ent_only=True)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collater)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collater)\n",
    "# tokens, bio_tags, entity_ids = next(iter(train_dataloader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b836c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define optimizer, model, loss etc. \n",
    "\n",
    "\n",
    "# with open(entity_embedding_metadata) as f:\n",
    "#     luke_metadata = json.loads(f.read())\n",
    "\n",
    "# luke_config = luke_metadata['model_config']\n",
    "# bert_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# config = LukeConfig(\n",
    "#     entity_vocab_size=luke_config['entity_vocab_size'],\n",
    "#     bert_model_name=luke_config['bert_model_name'],\n",
    "#     entity_emb_size=luke_config['entity_emb_size'],\n",
    "#     **bert_config.to_dict(),\n",
    "# )\n",
    "\n",
    "# luke_model = LukeModel(config)\n",
    "# pretrained_entity_embeddings = luke_model.load_state_dict(torch.load(entity_embedding_model, map_location=torch.device('cpu')))\n",
    "# print(pretrained_entity_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab795ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve pretrained entity embeddings\n",
    "\n",
    "model_archive = ModelArchive.load(entity_embedding_model)\n",
    "\n",
    "model_archive.tokenizer\n",
    "model_archive.entity_vocab\n",
    "model_archive.bert_model_name\n",
    "model_archive.config\n",
    "model_archive.max_mention_length\n",
    "pretrained_entity_embeddings = model_archive.state_dict['entity_embeddings.entity_embeddings.weight']\n",
    "pretrained_entity_embeddings = pretrained_entity_embeddings.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69155365",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0344d2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim_matrix(a, b, eps=1e-8):\n",
    "    \"\"\"\n",
    "    works on a 3D and 2D array\n",
    "    \"\"\"\n",
    "    a_n, b_n = torch.linalg.norm(a, dim=-1)[:, :, None], torch.linalg.norm(b, dim=-1)[:, None]\n",
    "    a_norm = a / torch.max(a_n, eps * torch.ones_like(a_n))\n",
    "    b_norm = b / torch.max(b_n, eps * torch.ones_like(b_n))\n",
    "    sim_mt = torch.matmul(a_norm, b_norm.transpose(0, 1))        \n",
    "    return sim_mt        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38b64c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define loss function\n",
    "def mention_entity_loss(mention_pred, entity_pred, bio_tags, entity_ids, \n",
    "                        attention_mask, pretrained_entity_embedding, device, lm=0.1):\n",
    "    # attention_mask torch.Size([27,256])\n",
    "    # mention_pred torch.Size([27, 256, 3])\n",
    "    # entity_pred torch.Size([27, 256, 256])\n",
    "    # bio_tags torch.Size([27, 256])\n",
    "    # entity_ids torch.Size([27, 256])\n",
    "    \n",
    "    # unpadded_men_pred torch.Size([25, 3, 256, 256])\n",
    "    # unpadded_bio_tags torch.Size([25, 256, 256])    \n",
    "    \n",
    "    ### MENTION LOSS\n",
    "\n",
    "    # first compute all loss\n",
    "    mention_loss = F.nll_loss(mention_pred.permute(0,2,1).contiguous(), bio_tags, reduction=\"none\")\n",
    "\n",
    "    # get only the unpadded losses: batch size\n",
    "    num_unpadded = torch.sum(attention_mask, dim=1)\n",
    "    masked_mention_loss = torch.where(attention_mask == 1, mention_loss, torch.tensor(0.).float().to(device))\n",
    "    \n",
    "    # compute average of the masked loss\n",
    "    avg_mention_loss = torch.sum(masked_mention_loss, dim=1) / num_unpadded\n",
    "    avg_mention_loss = torch.sum(avg_mention_loss)/len(avg_mention_loss)\n",
    "\n",
    "#     print(\"MENTION LOSS: \", avg_mention_loss)\n",
    "\n",
    "    \n",
    "    ### ENTITY LOSS\n",
    "#     cos_sim = sim_matrix(entity_pred, pretrained_entity_embedding)\n",
    "#     print(\"cos_sim\", cos_sim.shape)\n",
    "    # cos_sim torch.Size([27, 256, 34727])\n",
    "    \n",
    "    # get all the IDs in entity_ids that aren't -1\n",
    "    nonzero_ent_ids = entity_ids[entity_ids!=-1]\n",
    "    # get the true entity embeddings\n",
    "    # shape: [nonzero_ent_ids, embed_dim]\n",
    "    true_ent_embeddings = pretrained_entity_embedding[nonzero_ent_ids, :]\n",
    "    \n",
    "    # get non-neg1 idxs \n",
    "    ent_idxs = (entity_ids != -1).nonzero()\n",
    "    real_entity_pred = entity_pred[ent_idxs[:, 0], ent_idxs[:, 1], :]    \n",
    "#     print(\"true_ent_embeddings\", true_ent_embeddings.shape)\n",
    "#     print(real_entity_pred.shape)\n",
    "    entity_loss = torch.sum(1 - F.cosine_similarity(true_ent_embeddings, real_entity_pred, dim=1))/true_ent_embeddings.shape[1]\n",
    "#     print(\"ENTITY LOSS\", entity_loss)\n",
    "#     max_ents = torch.max(cos_sim, dim=2)\n",
    "    \n",
    "    # target_ent_embeddings = pretrained_entity_embedding[entity_ids, :]\n",
    "    # target_ent_embeddings torch.Size([27, 256, 256])\n",
    "    # max_ent_idxs = torch.argmax(cos_sim, dim=2)\n",
    "#     max_ents = self.entity_embedding[max_ent_idxs, :]\n",
    "    \n",
    "    \n",
    "#     entity_pred[ent_idxs]\n",
    "#     entity_loss = torch.mean((output - target)**2)\n",
    "    \n",
    "    \n",
    "\n",
    "    loss = (lm * avg_mention_loss) + ((1-lm) * entity_loss)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75fc624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a19eef7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# define model, etc. \n",
    "model = EntityLinker(model_name='microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract',\n",
    "                     pretrained_entity_embeddings=pretrained_entity_embeddings )\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.00002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2835a036",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad:\n",
    "#     print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f25c32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bi_spans(batch_tags):\n",
    "    \"\"\"Returns a list of N x 2 numpy arrays\"\"\"\n",
    "    batch_tags = batch_tags.cpu().detach().numpy()\n",
    "    bi_spans = []\n",
    "    for batch_idx in range(batch_tags.shape[0]):\n",
    "        b_tags = (batch_tags[batch_idx, :] == 0).astype(int)\n",
    "        i_tags = (batch_tags[batch_idx, :] == 1).astype(int)\n",
    "        \n",
    "        # if there aren't any beginning tags, just return an empty array\n",
    "        if not b_tags.any():\n",
    "            bi_spans.append(np.array([]))\n",
    "            continue\n",
    "        \n",
    "        # get all B tag locations\n",
    "        b_idx, = b_tags.nonzero() \n",
    "\n",
    "        # get idxs for I tags\n",
    "        d = np.diff(i_tags)\n",
    "        i_idx, = d.nonzero()\n",
    "\n",
    "        # We need to start things after the change in \"condition\". Therefore, \n",
    "        # we'll shift the index by 1 to the right.\n",
    "        i_idx += 1\n",
    "\n",
    "        # add end idxs for all B tags (just interweave +1 of b_idxs)\n",
    "        b_idx = np.vstack((b_idx,b_idx+1)).reshape((-1,),order='F')    \n",
    "\n",
    "        if i_tags[0]:\n",
    "            # If the start of condition is True prepend a 0\n",
    "            i_idx = np.r_[0, i_idx]\n",
    "\n",
    "        if i_tags[-1]:\n",
    "            # If the end of condition is True, append the length of the array\n",
    "            i_idx = np.r_[i_idx, i_tags.size] # Edit\n",
    "\n",
    "        # reshape to idxs\n",
    "        b_idx.shape = (-1,2)\n",
    "        i_idx.shape = (-1,2)    \n",
    "\n",
    "        bi_idx = []\n",
    "        # combine the b and i tags\n",
    "        for start, stop in b_idx:\n",
    "            # get idx where I tags start for each B tag, if exists\n",
    "            i, = np.where(i_idx[:, 0] == stop) \n",
    "            if i.size > 0:\n",
    "                bi_idx.append([start, int(i_idx[i, 1])])\n",
    "            else:\n",
    "                bi_idx.append([start, stop])\n",
    "\n",
    "        bi_spans.append(np.array(bi_idx))\n",
    "\n",
    "    assert batch_tags.shape[0] == len(bi_spans), f\"{batch_tags.shape[0]},{len(bi_spans)},{b_idx}\"\n",
    "    return bi_spans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "306a0937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_conf_matrices(mention_preds, entity_preds, bio_tags, entity_ids, pretrained_entity_embedding):\n",
    "    '''Computes both macro and micro confusion matrices given torch tensor outputs from our model\n",
    "    '''\n",
    "    # mention_pred Size([27, 256, 3])\n",
    "    # entity_pred torch.Size([27, 256, 256])\n",
    "\n",
    "    nb_classes = 2\n",
    "    micro_confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    # micro_confusion_matrix [[TP, FN], [FP, TN]]\n",
    "        \n",
    "    macro_confusion_matrix = []        \n",
    "    \n",
    "    # first we need to find the max of the BIO tagging we have\n",
    "    pred_bio_tags = torch.argmax(mention_preds, dim=2)\n",
    "    # pred_bio_tags torch.Size([28, 256])\n",
    "    \n",
    "    # create two lists of spans\n",
    "    pred_span_list = get_bi_spans(pred_bio_tags)\n",
    "    target_span_list = get_bi_spans(bio_tags)    \n",
    "\n",
    "    for batch_idx, (pred_spans, target_spans) in enumerate(zip(pred_span_list, target_span_list)):\n",
    "        tp_matches = 0\n",
    "        \n",
    "        batch_confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "\n",
    "        # do the STRONG MATCHING here\n",
    "\n",
    "        # first check if there are any predicted spans, if not, we just add a bunch of FNs\n",
    "        if pred_spans.size <= 0 or pred_spans.shape[0] == 0:\n",
    "            micro_confusion_matrix[0,1] += target_spans.shape[0]\n",
    "            batch_confusion_matrix[0,1] += target_spans.shape[0]\n",
    "            \n",
    "        # conversely, if the target spans are empty, we add a bunch of FPs instead\n",
    "        elif target_spans.size <= 0:\n",
    "            micro_confusion_matrix[1,0] += pred_spans.shape[0]\n",
    "            batch_confusion_matrix[1,0] += pred_spans.shape[0]            \n",
    "            \n",
    "        else:\n",
    "            for span_idx in range(pred_spans.shape[0]):\n",
    "                \n",
    "                if torch.all(torch.tensor(pred_spans[span_idx, :] == target_spans), dim=1).any():\n",
    "                    seq_idx = pred_spans[span_idx, 0]\n",
    "                    # we use the start span idx to get the entity embedding and compare it to the pretrained embeds\n",
    "\n",
    "                    cos_sim = sim_matrix(entity_preds[batch_idx, seq_idx, :].unsqueeze(0).unsqueeze(0), \n",
    "                                        pretrained_entity_embedding)\n",
    "                    # this should return something of size [1, 1, sim_vals]\n",
    "                    pred_ent_embed_id = torch.argmax(torch.squeeze(cos_sim))\n",
    "                    target_ent_embed_id = entity_ids[batch_idx, seq_idx]\n",
    "                    # just to test: \n",
    "                    # TODO: REMOVE\n",
    "    #                 if batch_idx == 1 and seq_idx == 0:\n",
    "    #                     target_ent_embed_id = 2\n",
    "                    # TODO: ENDREMOVE\n",
    "                    if pred_ent_embed_id == target_ent_embed_id:\n",
    "                        # TP\n",
    "                        micro_confusion_matrix[0,0] += 1\n",
    "                        batch_confusion_matrix[0,0] += 1 \n",
    "                        tp_matches += 1\n",
    "                    else:\n",
    "                        # FP\n",
    "                        micro_confusion_matrix[1,0] += 1\n",
    "                        batch_confusion_matrix[1,0] += 1\n",
    "                else:\n",
    "                    # FP \n",
    "                    micro_confusion_matrix[1,0] += 1\n",
    "                    batch_confusion_matrix[1,0] += 1\n",
    "        \n",
    "        # all FNs \n",
    "        micro_confusion_matrix[0,1] += target_spans.shape[0] - tp_matches\n",
    "        batch_confusion_matrix[0,1] += target_spans.shape[0] - tp_matches\n",
    "        \n",
    "        # we don't do TNs since we don't need them for P,R,F1\n",
    "        \n",
    "        # add on the batch confusion matrix to the batch\n",
    "        macro_confusion_matrix.append(batch_confusion_matrix)\n",
    "\n",
    "    return micro_confusion_matrix, macro_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46b9c557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(metrics, micro_conf, macro_confs):\n",
    "    '''Given both the micro and macro confusion matrices, we compute a given set of metrics\n",
    "    \n",
    "    Matrix orientation is: \n",
    "    |  TP  |  FN |\n",
    "    |-------------\n",
    "    |  FP  |  TN |\n",
    "    '''\n",
    "    micro_metrics = {}\n",
    "    macro_metrics = {}\n",
    "    \n",
    "    if \"precision\" in metrics:\n",
    "        micro_metrics['precision'] = micro_conf[0,0] / (micro_conf[0,0] + micro_conf[1,0])\n",
    "        macro_metrics['precision'] = torch.mean(torch.tensor([batch_conf[0,0] / (batch_conf[0,0] + batch_conf[1,0]) for batch_conf in macro_confs]))\n",
    "    if \"recall\" in metrics:\n",
    "        micro_metrics['recall'] = micro_conf[0,0] / (micro_conf[0,0] + micro_conf[0,1])\n",
    "        macro_metrics['recall'] = torch.mean(torch.tensor([batch_conf[0,0] / (batch_conf[0,0] + batch_conf[0,1]) for batch_conf in macro_confs]))\n",
    "    if \"f1\" in metrics:\n",
    "        micro_metrics['f1'] = micro_conf[0,0] / (micro_conf[0,0] + ((1/2) * (micro_conf[0,1] + micro_conf[1,0])))\n",
    "        macro_metrics['f1'] = torch.mean(torch.tensor([batch_conf[0,0] / (batch_conf[0,0] + ((1/2) * (batch_conf[0,1] + batch_conf[1,0]))) for batch_conf in macro_confs]))\n",
    "        \n",
    "    return micro_metrics, macro_metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0a74710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train!!\n",
    "def train(model, train_loader, optimizer, criterion):\n",
    "\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        tokenized_text, bio_tags, entity_ids = data\n",
    "        bio_tags = bio_tags.to(device)\n",
    "        entity_ids = entity_ids.to(device)\n",
    "#         print(\"bio_tags\", bio_tags.shape)\n",
    "#         print(\"entity_ids\", entity_ids.shape)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "#         with autograd.detect_anomaly():\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        input_ids = tokenized_text['input_ids'].to(device)\n",
    "        token_type_ids = tokenized_text['token_type_ids'].to(device)\n",
    "        attention_mask = tokenized_text['attention_mask'].to(device)\n",
    "                          \n",
    "        mention_pred, entity_pred = model(input_ids,token_type_ids,attention_mask)\n",
    "#         print(\"ent pred\", entity_pred.shape)\n",
    "        loss = criterion(mention_pred, entity_pred, bio_tags, entity_ids, attention_mask, \n",
    "                         pretrained_entity_embeddings, device)\n",
    "#         print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 5 == 4:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] train loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 5))\n",
    "            running_loss = 0.0\n",
    "#         if i == 2:\n",
    "#             break\n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64003d09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "397574aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_loader, optimizer, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            \n",
    "            tokenized_text, bio_tags, entity_ids = data\n",
    "            bio_tags = bio_tags.to(device)\n",
    "            entity_ids = entity_ids.to(device)\n",
    "            \n",
    "    #         with autograd.detect_anomaly():\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            input_ids = tokenized_text['input_ids'].to(device)\n",
    "            token_type_ids = tokenized_text['token_type_ids'].to(device)\n",
    "            attention_mask = tokenized_text['attention_mask'].to(device)\n",
    "\n",
    "            mention_pred, entity_pred = model(input_ids,token_type_ids,attention_mask)\n",
    "    #         print(\"ent pred\", entity_pred.shape)\n",
    "            loss = criterion(mention_pred, entity_pred, bio_tags, entity_ids, \n",
    "                            attention_mask, pretrained_entity_embeddings, device)\n",
    "    #         print(loss)\n",
    "\n",
    "\n",
    "            micro_conf_mat, macro_conf_mat = compute_conf_matrices(mention_pred, entity_pred, bio_tags, entity_ids, pretrained_entity_embeddings)\n",
    "            micro_metrics, macro_metrics = compute_metrics([\"precision\", \"recall\",\"f1\"], micro_conf_mat, macro_conf_mat)\n",
    "            print('-' * 89)\n",
    "            print(f\"Micro-Precision: {micro_metrics['precision']}, Macro-Precision: {macro_metrics['precision']}\\n\")\n",
    "            print(f\"Micro-Recall: {micro_metrics['recall']}, Macro-Recall: {macro_metrics['recall']}\\n\")\n",
    "            print(f\"Micro-F1: {micro_metrics['f1']}, Macro-F1: {macro_metrics['f1']}\\n\")    \n",
    "            print('-' * 89)\n",
    "\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            if i % 5 == 4:    # print every 2000 mini-batches\n",
    "                print('[%d, %5d] test loss: %.3f' %\n",
    "                      (epoch + 1, i + 1, running_loss / 5))\n",
    "                running_loss = 0.0\n",
    "        # whatever i is at is the len of test_loader\n",
    "        return total_loss / i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cf2a87da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     5] loss: 6.795\n",
      "[1,    10] loss: 6.572\n",
      "[1,    15] loss: 6.732\n",
      "[1,    20] loss: 6.706\n",
      "[1,    25] loss: 6.600\n",
      "[1,    30] loss: 6.564\n",
      "[1,    35] loss: 6.701\n",
      "[1,    40] loss: 6.623\n",
      "[1,    45] loss: 6.565\n",
      "[1,    50] loss: 6.784\n",
      "[1,    55] loss: 6.433\n",
      "[1,    60] loss: 6.522\n",
      "[1,    65] loss: 6.760\n",
      "[1,    70] loss: 6.644\n",
      "[1,    75] loss: 6.451\n",
      "[1,    80] loss: 6.339\n",
      "Finished Training\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2070000022649765, Macro-Precision: 0.19737441837787628\n",
      "\n",
      "Micro-Recall: 0.2449704110622406, Macro-Recall: 0.22900573909282684\n",
      "\n",
      "Micro-F1: 0.22439023852348328, Macro-F1: 0.20945365726947784\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.1898948848247528, Macro-Precision: 0.19537004828453064\n",
      "\n",
      "Micro-Recall: 0.22125642001628876, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.20437955856323242, Macro-F1: 0.19806745648384094\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.17730769515037537, Macro-Precision: 0.1806885004043579\n",
      "\n",
      "Micro-Recall: 0.2132284939289093, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.19361612200737, Macro-F1: 0.1884831339120865\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.1846473067998886, Macro-Precision: 0.19937744736671448\n",
      "\n",
      "Micro-Recall: 0.20957614481449127, Macro-Recall: 0.21514897048473358\n",
      "\n",
      "Micro-F1: 0.19632352888584137, Macro-F1: 0.2028391808271408\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.17656821012496948, Macro-Precision: 0.1812618523836136\n",
      "\n",
      "Micro-Recall: 0.22881720960140228, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.1993255913257599, Macro-F1: 0.19127269089221954\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[1,     5] train loss: 6.706\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.1896420419216156, Macro-Precision: 0.1987156718969345\n",
      "\n",
      "Micro-Recall: 0.23919308185577393, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.21155479550361633, Macro-F1: 0.21072399616241455\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.20342133939266205, Macro-Precision: 0.1994248330593109\n",
      "\n",
      "Micro-Recall: 0.2651478350162506, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.23021885752677917, Macro-F1: 0.21514491736888885\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2675745189189911, Macro-Precision: 0.26299425959587097\n",
      "\n",
      "Micro-Recall: 0.30494967103004456, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2850421369075775, Macro-F1: 0.27481919527053833\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23014847934246063, Macro-Precision: 0.22936272621154785\n",
      "\n",
      "Micro-Recall: 0.2746533155441284, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2504390478134155, Macro-F1: 0.24202074110507965\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19304005801677704, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2297772616147995, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.20981267094612122, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[1,    10] train loss: 6.313\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.21647708117961884, Macro-Precision: 0.22286725044250488\n",
      "\n",
      "Micro-Recall: 0.24950337409973145, Macro-Recall: 0.2581295371055603\n",
      "\n",
      "Micro-F1: 0.23181985318660736, Macro-F1: 0.23579129576683044\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.1940401941537857, Macro-Precision: 0.1894405633211136\n",
      "\n",
      "Micro-Recall: 0.2259887009859085, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.20879940688610077, Macro-F1: 0.19793738424777985\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19201388955116272, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2332349270582199, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2106265425682068, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22161641716957092, Macro-Precision: 0.22139985859394073\n",
      "\n",
      "Micro-Recall: 0.26638397574424744, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2419467717409134, Macro-F1: 0.23119193315505981\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2030378133058548, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2289409339427948, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.21521274745464325, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[1,    15] train loss: 7.007\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19531822204589844, Macro-Precision: 0.18464654684066772\n",
      "\n",
      "Micro-Recall: 0.22928296029567719, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2109421342611313, Macro-F1: 0.18974927067756653\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2199462652206421, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2454102635383606, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.23198159039020538, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.18338845670223236, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.22237077355384827, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.20100705325603485, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19437777996063232, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.21707503497600555, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.20510037243366241, Macro-F1: 0.19467589259147644\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2251698523759842, Macro-Precision: 0.22523196041584015\n",
      "\n",
      "Micro-Recall: 0.26383623480796814, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.24297434091567993, Macro-F1: 0.229519322514534\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[1,    20] train loss: 6.845\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.20210598409175873, Macro-Precision: 0.19366760551929474\n",
      "\n",
      "Micro-Recall: 0.23508495092391968, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.21735160052776337, Macro-F1: 0.20283766090869904\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2083616554737091, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.24229249358177185, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.22404970228672028, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.21802857518196106, Macro-Precision: 0.22363512217998505\n",
      "\n",
      "Micro-Recall: 0.23962947726249695, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.22831925749778748, Macro-F1: 0.2247065305709839\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2053571492433548, Macro-Precision: 0.2081628292798996\n",
      "\n",
      "Micro-Recall: 0.2524698078632355, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.22648940980434418, Macro-F1: 0.21811993420124054\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19710859656333923, Macro-Precision: 0.19016139209270477\n",
      "\n",
      "Micro-Recall: 0.23418517410755157, Macro-Recall: 0.22046706080436707\n",
      "\n",
      "Micro-F1: 0.2140532284975052, Macro-F1: 0.2016618847846985\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[1,    25] train loss: 6.912\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.18121029436588287, Macro-Precision: 0.18034936487674713\n",
      "\n",
      "Micro-Recall: 0.21105918288230896, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.194999098777771, Macro-F1: 0.19017340242862701\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22883065044879913, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.24629294872283936, Macro-Recall: 0.23366761207580566\n",
      "\n",
      "Micro-F1: 0.23724089562892914, Macro-F1: 0.22448422014713287\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19680464267730713, Macro-Precision: 0.21244263648986816\n",
      "\n",
      "Micro-Recall: 0.21784566342830658, Macro-Recall: 0.2037915736436844\n",
      "\n",
      "Micro-F1: 0.20679129660129547, Macro-F1: 0.20360223948955536\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   0 | time: 105.87s | valid loss  6.93 | valid ppl  1021.43\n",
      "-----------------------------------------------------------------------------------------\n",
      "[2,     5] loss: 6.217\n",
      "[2,    10] loss: 6.405\n",
      "[2,    15] loss: 6.272\n",
      "[2,    20] loss: 6.200\n",
      "[2,    25] loss: 6.272\n",
      "[2,    30] loss: 6.202\n",
      "[2,    35] loss: 6.342\n",
      "[2,    40] loss: 6.252\n",
      "[2,    45] loss: 6.058\n",
      "[2,    50] loss: 6.495\n",
      "[2,    55] loss: 6.300\n",
      "[2,    60] loss: 6.172\n",
      "[2,    65] loss: 6.186\n",
      "[2,    70] loss: 6.111\n",
      "[2,    75] loss: 6.347\n",
      "[2,    80] loss: 6.229\n",
      "Finished Training\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23712901771068573, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2997702956199646, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2647953927516937, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19602181017398834, Macro-Precision: 0.1959867626428604\n",
      "\n",
      "Micro-Recall: 0.25944799184799194, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.22331871092319489, Macro-F1: 0.20927059650421143\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22127936780452728, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.26289260387420654, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.24029771983623505, Macro-F1: 0.22703900933265686\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24210186302661896, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2995612323284149, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.26778391003608704, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23016393184661865, Macro-Precision: 0.2284896820783615\n",
      "\n",
      "Micro-Recall: 0.2921348214149475, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.257472962141037, Macro-F1: 0.25214865803718567\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[2,     5] train loss: 6.669\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.21307691931724548, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.268410861492157, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2375643253326416, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24923454225063324, Macro-Precision: 0.25176024436950684\n",
      "\n",
      "Micro-Recall: 0.3064759075641632, Macro-Recall: 0.3077629506587982\n",
      "\n",
      "Micro-F1: 0.27490711212158203, Macro-F1: 0.2721501290798187\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19971969723701477, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2757619619369507, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.23166023194789886, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23345288634300232, Macro-Precision: 0.2317127138376236\n",
      "\n",
      "Micro-Recall: 0.2769826054573059, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.25336164236068726, Macro-F1: 0.23998647928237915\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23373651504516602, Macro-Precision: 0.24712802469730377\n",
      "\n",
      "Micro-Recall: 0.2670900225639343, Macro-Recall: 0.2617904245853424\n",
      "\n",
      "Micro-F1: 0.24930265545845032, Macro-F1: 0.24836598336696625\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[2,    10] train loss: 6.397\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24565669894218445, Macro-Precision: 0.2415967583656311\n",
      "\n",
      "Micro-Recall: 0.2891615629196167, Macro-Recall: 0.27835550904273987\n",
      "\n",
      "Micro-F1: 0.26563966274261475, Macro-F1: 0.256385862827301\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.21952079236507416, Macro-Precision: 0.21556943655014038\n",
      "\n",
      "Micro-Recall: 0.2654452621936798, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.24030858278274536, Macro-F1: 0.228326216340065\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2342139184474945, Macro-Precision: 0.2350863814353943\n",
      "\n",
      "Micro-Recall: 0.29480940103530884, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2610412836074829, Macro-F1: 0.2514399588108063\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2333010584115982, Macro-Precision: 0.23261867463588715\n",
      "\n",
      "Micro-Recall: 0.2594187259674072, Macro-Recall: 0.25338107347488403\n",
      "\n",
      "Micro-F1: 0.24566768109798431, Macro-F1: 0.24012070894241333\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2131415456533432, Macro-Precision: 0.2123514860868454\n",
      "\n",
      "Micro-Recall: 0.2540919780731201, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.23182222247123718, Macro-F1: 0.22231332957744598\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[2,    15] train loss: 6.718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23054379224777222, Macro-Precision: 0.2360098659992218\n",
      "\n",
      "Micro-Recall: 0.29075974225997925, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.25717398524284363, Macro-F1: 0.24991273880004883\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24352671205997467, Macro-Precision: 0.23651044070720673\n",
      "\n",
      "Micro-Recall: 0.2991143465042114, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.268473356962204, Macro-F1: 0.25186842679977417\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.257432222366333, Macro-Precision: 0.24168016016483307\n",
      "\n",
      "Micro-Recall: 0.30926215648651123, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2809770107269287, Macro-F1: 0.258938729763031\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22476759552955627, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2825542688369751, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2503698170185089, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.255982369184494, Macro-Precision: 0.2571922540664673\n",
      "\n",
      "Micro-Recall: 0.3093607425689697, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2801516056060791, Macro-F1: 0.2731091380119324\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[2,    20] train loss: 6.592\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23198197782039642, Macro-Precision: 0.2510493993759155\n",
      "\n",
      "Micro-Recall: 0.27848589420318604, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2531156837940216, Macro-F1: 0.26295793056488037\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2732067406177521, Macro-Precision: 0.2570571005344391\n",
      "\n",
      "Micro-Recall: 0.31104883551597595, Macro-Recall: 0.29332712292671204\n",
      "\n",
      "Micro-F1: 0.2909022867679596, Macro-F1: 0.2695609927177429\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22988881170749664, Macro-Precision: 0.2248532772064209\n",
      "\n",
      "Micro-Recall: 0.28153783082962036, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2531053125858307, Macro-F1: 0.23372992873191833\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23641304671764374, Macro-Precision: 0.2300383746623993\n",
      "\n",
      "Micro-Recall: 0.28807947039604187, Macro-Recall: 0.2763608992099762\n",
      "\n",
      "Micro-F1: 0.2597014904022217, Macro-F1: 0.24903854727745056\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2564575672149658, Macro-Precision: 0.2576295733451843\n",
      "\n",
      "Micro-Recall: 0.3079763650894165, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2798657715320587, Macro-F1: 0.2733485996723175\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[2,    25] train loss: 6.830\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22450917959213257, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.272482693195343, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.24618054926395416, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.21441707015037537, Macro-Precision: 0.21130894124507904\n",
      "\n",
      "Micro-Recall: 0.26928406953811646, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2387387454509735, Macro-F1: 0.22443698346614838\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.19512194395065308, Macro-Precision: 0.19415071606636047\n",
      "\n",
      "Micro-Recall: 0.2431865781545639, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.21651889383792877, Macro-F1: 0.20107269287109375\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   1 | time: 107.56s | valid loss  6.72 | valid ppl   829.27\n",
      "-----------------------------------------------------------------------------------------\n",
      "[3,     5] loss: 5.829\n",
      "[3,    10] loss: 5.831\n",
      "[3,    15] loss: 6.136\n",
      "[3,    20] loss: 6.347\n",
      "[3,    25] loss: 5.728\n",
      "[3,    30] loss: 5.880\n",
      "[3,    35] loss: 6.089\n",
      "[3,    40] loss: 6.006\n",
      "[3,    45] loss: 6.088\n",
      "[3,    50] loss: 5.746\n",
      "[3,    55] loss: 6.061\n",
      "[3,    60] loss: 6.098\n",
      "[3,    65] loss: 5.970\n",
      "[3,    70] loss: 6.116\n",
      "[3,    75] loss: 5.736\n",
      "[3,    80] loss: 6.083\n",
      "Finished Training\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24398395419120789, Macro-Precision: 0.24108651280403137\n",
      "\n",
      "Micro-Recall: 0.3134392499923706, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2743845283985138, Macro-F1: 0.26389774680137634\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.27374470233917236, Macro-Precision: 0.27198469638824463\n",
      "\n",
      "Micro-Recall: 0.34901657700538635, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3068316578865051, Macro-F1: 0.2904857099056244\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.26096490025520325, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2923832833766937, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.27578216791152954, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2539682686328888, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.29124709963798523, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2713332176208496, Macro-F1: 0.2523671090602875\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.25996649265289307, Macro-Precision: 0.26646944880485535\n",
      "\n",
      "Micro-Recall: 0.3176422417163849, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2859248220920563, Macro-F1: 0.28132131695747375\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[3,     5] train loss: 6.353\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22052808105945587, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.277881920337677, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2459050416946411, Macro-F1: 0.22391052544116974\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22937293350696564, Macro-Precision: 0.22090931236743927\n",
      "\n",
      "Micro-Recall: 0.2784455120563507, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2515381872653961, Macro-F1: 0.2304905205965042\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23356401920318604, Macro-Precision: 0.2430764138698578\n",
      "\n",
      "Micro-Recall: 0.28313758969306946, Macro-Recall: 0.27848678827285767\n",
      "\n",
      "Micro-F1: 0.255972683429718, Macro-F1: 0.25502708554267883\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.27583643794059753, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.3206568658351898, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2965627610683441, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2989655137062073, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.338143527507782, Macro-Recall: 0.3173072934150696\n",
      "\n",
      "Micro-F1: 0.31734994053840637, Macro-F1: 0.29473164677619934\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[3,    10] train loss: 6.137\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.23402909934520721, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.28582465648651123, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2573465406894684, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2057921588420868, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.253462016582489, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.22715306282043457, Macro-F1: 0.22198577225208282\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.25429436564445496, Macro-Precision: 0.2510606050491333\n",
      "\n",
      "Micro-Recall: 0.29263564944267273, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2721211016178131, Macro-F1: 0.2634935975074768\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24961479008197784, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.28491029143333435, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2660972476005554, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.25969529151916504, Macro-Precision: 0.2550598084926605\n",
      "\n",
      "Micro-Recall: 0.29856687784194946, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2777777910232544, Macro-F1: 0.26882117986679077\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[3,    15] train loss: 6.773\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2563853859901428, Macro-Precision: 0.255111962556839\n",
      "\n",
      "Micro-Recall: 0.2865919768810272, Macro-Recall: 0.278647243976593\n",
      "\n",
      "Micro-F1: 0.27064844965934753, Macro-F1: 0.2640426456928253\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2261521965265274, Macro-Precision: 0.2315523475408554\n",
      "\n",
      "Micro-Recall: 0.2728448212146759, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.24731393158435822, Macro-F1: 0.24319811165332794\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2514200210571289, Macro-Precision: 0.2530885338783264\n",
      "\n",
      "Micro-Recall: 0.29759377241134644, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.27256521582603455, Macro-F1: 0.26363813877105713\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.25348374247550964, Macro-Precision: 0.2656746208667755\n",
      "\n",
      "Micro-Recall: 0.31466227769851685, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2807791233062744, Macro-F1: 0.2846963107585907\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2597488462924957, Macro-Precision: 0.25409626960754395\n",
      "\n",
      "Micro-Recall: 0.31252485513687134, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2837032973766327, Macro-F1: 0.26982977986335754\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[3,    20] train loss: 6.659\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.25771448016166687, Macro-Precision: 0.25990036129951477\n",
      "\n",
      "Micro-Recall: 0.3210815489292145, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.28592926263809204, Macro-F1: 0.2756551206111908\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2483339160680771, Macro-Precision: 0.24626658856868744\n",
      "\n",
      "Micro-Recall: 0.30876579880714417, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2752721607685089, Macro-F1: 0.2604166567325592\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2542201280593872, Macro-Precision: 0.2521072030067444\n",
      "\n",
      "Micro-Recall: 0.2980997562408447, Macro-Recall: 0.2929229438304901\n",
      "\n",
      "Micro-F1: 0.2744169235229492, Macro-F1: 0.26784613728523254\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2373853176832199, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.29798465967178345, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2642553150653839, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.26244056224823, Macro-Precision: 0.26361772418022156\n",
      "\n",
      "Micro-Recall: 0.28951048851013184, Macro-Recall: 0.2826041877269745\n",
      "\n",
      "Micro-F1: 0.2753117084503174, Macro-F1: 0.2688826024532318\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[3,    25] train loss: 6.272\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.21763916313648224, Macro-Precision: 0.21576052904129028\n",
      "\n",
      "Micro-Recall: 0.27086812257766724, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.24135366082191467, Macro-F1: 0.23305319249629974\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.253778338432312, Macro-Precision: 0.249289408326149\n",
      "\n",
      "Micro-Recall: 0.31300970911979675, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2802990674972534, Macro-F1: 0.2645643949508667\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.3140625059604645, Macro-Precision: 0.32079195976257324\n",
      "\n",
      "Micro-Recall: 0.3366834223270416, Macro-Recall: 0.3397398591041565\n",
      "\n",
      "Micro-F1: 0.3249797821044922, Macro-F1: 0.3274000585079193\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   2 | time: 107.58s | valid loss  6.56 | valid ppl   705.85\n",
      "-----------------------------------------------------------------------------------------\n",
      "[4,     5] loss: 5.747\n",
      "[4,    10] loss: 6.029\n",
      "[4,    15] loss: 5.642\n",
      "[4,    20] loss: 5.921\n",
      "[4,    25] loss: 6.005\n",
      "[4,    30] loss: 5.968\n",
      "[4,    35] loss: 5.581\n",
      "[4,    40] loss: 5.629\n",
      "[4,    45] loss: 5.573\n",
      "[4,    50] loss: 5.863\n",
      "[4,    55] loss: 5.629\n",
      "[4,    60] loss: 5.652\n",
      "[4,    65] loss: 5.650\n",
      "[4,    70] loss: 5.832\n",
      "[4,    75] loss: 5.682\n",
      "[4,    80] loss: 5.865\n",
      "Finished Training\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.26767677068710327, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.3051372766494751, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2851821184158325, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2523461878299713, Macro-Precision: 0.25624117255210876\n",
      "\n",
      "Micro-Recall: 0.3058129847049713, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2765187621116638, Macro-F1: 0.26488667726516724\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2629427909851074, Macro-Precision: 0.2652309536933899\n",
      "\n",
      "Micro-Recall: 0.30720254778862, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2833547294139862, Macro-F1: 0.27405864000320435\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2319551557302475, Macro-Precision: 0.24153731763362885\n",
      "\n",
      "Micro-Recall: 0.27687159180641174, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.25243088603019714, Macro-F1: 0.24617533385753632\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2611487805843353, Macro-Precision: 0.25981566309928894\n",
      "\n",
      "Micro-Recall: 0.2920989692211151, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.27575814723968506, Macro-F1: 0.264455109834671\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[4,     5] train loss: 6.121\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2692967355251312, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.2987062335014343, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.28324010968208313, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2796330153942108, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.3003547489643097, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2896237075328827, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.28479817509651184, Macro-Precision: 0.28840768337249756\n",
      "\n",
      "Micro-Recall: 0.31159719824790955, Macro-Recall: 0.31256142258644104\n",
      "\n",
      "Micro-F1: 0.2975955903530121, Macro-F1: 0.29521045088768005\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2587733566761017, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.29988598823547363, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.27781689167022705, Macro-F1: 0.25644659996032715\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2474144846200943, Macro-Precision: 0.24520288407802582\n",
      "\n",
      "Micro-Recall: 0.2854520380496979, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2650756537914276, Macro-F1: 0.25428006052970886\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[4,    10] train loss: 6.276\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.26113566756248474, Macro-Precision: 0.25174617767333984\n",
      "\n",
      "Micro-Recall: 0.2987164556980133, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2786647379398346, Macro-F1: 0.264787495136261\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2689235508441925, Macro-Precision: 0.2679699659347534\n",
      "\n",
      "Micro-Recall: 0.28845369815826416, Macro-Recall: 0.28475016355514526\n",
      "\n",
      "Micro-F1: 0.278346449136734, Macro-F1: 0.2739188075065613\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2844488322734833, Macro-Precision: 0.2854231894016266\n",
      "\n",
      "Micro-Recall: 0.3180484175682068, Macro-Recall: 0.31064584851264954\n",
      "\n",
      "Micro-F1: 0.30031174421310425, Macro-F1: 0.2922900319099426\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2683464586734772, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.30614444613456726, Macro-Recall: 0.29641181230545044\n",
      "\n",
      "Micro-F1: 0.2860020101070404, Macro-F1: 0.27433356642723083\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.26812130212783813, Macro-Precision: 0.28942474722862244\n",
      "\n",
      "Micro-Recall: 0.2780974209308624, Macro-Recall: 0.2981690764427185\n",
      "\n",
      "Micro-F1: 0.2730182707309723, Macro-F1: 0.28957313299179077\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[4,    15] train loss: 6.707\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2786247134208679, Macro-Precision: 0.2785002291202545\n",
      "\n",
      "Micro-Recall: 0.3112318813800812, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.29402703046798706, Macro-F1: 0.2811967134475708\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.28396323323249817, Macro-Precision: 0.2737184166908264\n",
      "\n",
      "Micro-Recall: 0.34082549810409546, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.30980682373046875, Macro-F1: 0.2803304195404053\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24479541182518005, Macro-Precision: 0.23898477852344513\n",
      "\n",
      "Micro-Recall: 0.28205129504203796, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2621060609817505, Macro-F1: 0.2456435263156891\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2825189232826233, Macro-Precision: 0.28245848417282104\n",
      "\n",
      "Micro-Recall: 0.3276137411594391, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.30339986085891724, Macro-F1: 0.2941754460334778\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.29839298129081726, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.32890498638153076, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3129069209098816, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[4,    20] train loss: 6.340\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2584134638309479, Macro-Precision: 0.25069519877433777\n",
      "\n",
      "Micro-Recall: 0.2837659418582916, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2704969644546509, Macro-F1: 0.2583085298538208\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2995828688144684, Macro-Precision: 0.2956946790218353\n",
      "\n",
      "Micro-Recall: 0.32179227471351624, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.310290664434433, Macro-F1: 0.29555997252464294\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2612973749637604, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.3069349229335785, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2822834551334381, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2528553307056427, Macro-Precision: 0.24441951513290405\n",
      "\n",
      "Micro-Recall: 0.29562315344810486, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2725718319416046, Macro-F1: 0.2563508152961731\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.22533711791038513, Macro-Precision: 0.2319248765707016\n",
      "\n",
      "Micro-Recall: 0.2626137435436249, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.24255156517028809, Macro-F1: 0.2389853298664093\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[4,    25] train loss: 6.264\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2652428448200226, Macro-Precision: 0.26159578561782837\n",
      "\n",
      "Micro-Recall: 0.3087409734725952, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2853437066078186, Macro-F1: 0.2680239677429199\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2682165205478668, Macro-Precision: 0.2577258348464966\n",
      "\n",
      "Micro-Recall: 0.3165438175201416, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.29038316011428833, Macro-F1: 0.27278953790664673\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2396496832370758, Macro-Precision: 0.23106679320335388\n",
      "\n",
      "Micro-Recall: 0.29054054617881775, Macro-Recall: 0.2730080485343933\n",
      "\n",
      "Micro-F1: 0.2626526951789856, Macro-F1: 0.24904802441596985\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   3 | time: 107.02s | valid loss  6.44 | valid ppl   624.18\n",
      "-----------------------------------------------------------------------------------------\n",
      "[5,     5] loss: 5.512\n",
      "[5,    10] loss: 5.445\n",
      "[5,    15] loss: 5.727\n",
      "[5,    20] loss: 5.648\n",
      "[5,    25] loss: 5.546\n",
      "[5,    30] loss: 5.540\n",
      "[5,    35] loss: 5.707\n",
      "[5,    40] loss: 5.398\n",
      "[5,    45] loss: 5.725\n",
      "[5,    50] loss: 5.616\n",
      "[5,    55] loss: 5.450\n",
      "[5,    60] loss: 5.654\n",
      "[5,    65] loss: 5.818\n",
      "[5,    70] loss: 5.473\n",
      "[5,    75] loss: 5.253\n",
      "[5,    80] loss: 5.525\n",
      "Finished Training\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.28515318036079407, Macro-Precision: 0.2865622639656067\n",
      "\n",
      "Micro-Recall: 0.3271743953227997, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3047219216823578, Macro-F1: 0.29323717951774597\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2426973283290863, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.30119553208351135, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2688005566596985, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2562180459499359, Macro-Precision: 0.2666870653629303\n",
      "\n",
      "Micro-Recall: 0.30718955397605896, Macro-Recall: 0.30338647961616516\n",
      "\n",
      "Micro-F1: 0.279398113489151, Macro-F1: 0.27986106276512146\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.28510788083076477, Macro-Precision: 0.2925921678543091\n",
      "\n",
      "Micro-Recall: 0.3261837363243103, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3042657673358917, Macro-F1: 0.30255240201950073\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2870537340641022, Macro-Precision: 0.2835690677165985\n",
      "\n",
      "Micro-Recall: 0.30627164244651794, Macro-Recall: 0.3108188509941101\n",
      "\n",
      "Micro-F1: 0.29635146260261536, Macro-F1: 0.29107892513275146\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[5,     5] train loss: 6.207\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.25915491580963135, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.3045097291469574, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.28000760078430176, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.26715946197509766, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.31533029675483704, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2892530560493469, Macro-F1: 0.28300848603248596\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.29888656735420227, Macro-Precision: 0.29825738072395325\n",
      "\n",
      "Micro-Recall: 0.3367306888103485, Macro-Recall: 0.32801178097724915\n",
      "\n",
      "Micro-F1: 0.31668204069137573, Macro-F1: 0.3096083104610443\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2768439054489136, Macro-Precision: 0.28638508915901184\n",
      "\n",
      "Micro-Recall: 0.3091953992843628, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.29212668538093567, Macro-F1: 0.28527823090553284\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.29045775532722473, Macro-Precision: 0.29108184576034546\n",
      "\n",
      "Micro-Recall: 0.3515411615371704, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.31809356808662415, Macro-F1: 0.31005993485450745\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[5,    10] train loss: 6.239\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2702012062072754, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.29883432388305664, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.28379738330841064, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2870912253856659, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.32718712091445923, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.30583059787750244, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.3592090904712677, Macro-Precision: 0.35472676157951355\n",
      "\n",
      "Micro-Recall: 0.37159091234207153, Macro-Recall: 0.36023226380348206\n",
      "\n",
      "Micro-F1: 0.3652951121330261, Macro-F1: 0.3519589900970459\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2791193127632141, Macro-Precision: 0.27694210410118103\n",
      "\n",
      "Micro-Recall: 0.33647260069847107, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3051242232322693, Macro-F1: 0.2951054871082306\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2625215947628021, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.3029095232486725, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.28127312660217285, Macro-F1: 0.2548874318599701\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[5,    15] train loss: 6.375\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.25685691833496094, Macro-Precision: 0.25946852564811707\n",
      "\n",
      "Micro-Recall: 0.3038141131401062, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.27836915850639343, Macro-F1: 0.2697015404701233\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2190951257944107, Macro-Precision: 0.2171783447265625\n",
      "\n",
      "Micro-Recall: 0.25796979665756226, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.23694856464862823, Macro-F1: 0.22522838413715363\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24384485185146332, Macro-Precision: 0.23359854519367218\n",
      "\n",
      "Micro-Recall: 0.3017529249191284, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2697257995605469, Macro-F1: 0.2430112659931183\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.29777488112449646, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.3276917636394501, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3120178282260895, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2517586052417755, Macro-Precision: 0.23856154084205627\n",
      "\n",
      "Micro-Recall: 0.30276045203208923, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.27491408586502075, Macro-F1: 0.25013214349746704\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[5,    20] train loss: 6.166\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.29284238815307617, Macro-Precision: 0.2814849317073822\n",
      "\n",
      "Micro-Recall: 0.3202860355377197, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.30595001578330994, Macro-F1: 0.2933366298675537\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2521921992301941, Macro-Precision: 0.24202971160411835\n",
      "\n",
      "Micro-Recall: 0.2984640896320343, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2733840346336365, Macro-F1: 0.2551753520965576\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.3029578626155853, Macro-Precision: 0.30790141224861145\n",
      "\n",
      "Micro-Recall: 0.33642998337745667, Macro-Recall: 0.3354071378707886\n",
      "\n",
      "Micro-F1: 0.31881779432296753, Macro-F1: 0.31826257705688477\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.29898402094841003, Macro-Precision: nan\n",
      "\n",
      "Micro-Recall: 0.35273972153663635, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3236449360847473, Macro-F1: nan\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.30907753109931946, Macro-Precision: 0.2972756028175354\n",
      "\n",
      "Micro-Recall: 0.3499791920185089, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3282591700553894, Macro-F1: 0.3060617744922638\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "[5,    25] train loss: 6.325\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.24649962782859802, Macro-Precision: 0.24295006692409515\n",
      "\n",
      "Micro-Recall: 0.29252296686172485, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.26754650473594666, Macro-F1: 0.2551017701625824\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2757510840892792, Macro-Precision: 0.273495078086853\n",
      "\n",
      "Micro-Recall: 0.32111620903015137, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.2967096269130707, Macro-F1: 0.28450649976730347\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "Micro-Precision: 0.2703557312488556, Macro-Precision: 0.2792365550994873\n",
      "\n",
      "Micro-Recall: 0.3406374454498291, Macro-Recall: nan\n",
      "\n",
      "Micro-F1: 0.3014543950557709, Macro-F1: 0.28961262106895447\n",
      "\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| end of epoch   4 | time: 107.32s | valid loss  6.33 | valid ppl   560.12\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "import time\n",
    "import math \n",
    "\n",
    "# try:\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    epoch_start_time = time.time()\n",
    "    train(model, train_dataloader, optimizer, mention_entity_loss)\n",
    "    val_loss = evaluate(model, test_dataloader, optimizer, mention_entity_loss)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | valid ppl {:8.2f}'.format(epoch, \n",
    "                                      (time.time() - epoch_start_time),\n",
    "                                       val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)        \n",
    "# except Exception as e:\n",
    "#     print(\"An exception occurred\", e) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42dbc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2,3,5)\n",
    "m = torch.nn.Softmax(dim=2)\n",
    "print(x.shape)\n",
    "m(x)\n",
    "# torch.nn.Softmax(x, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "cos(input1, input2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47b1256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6311df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1511020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in1 shape [batch_size, seq_len, embed_size]\n",
    "# in1 shape [16, 256, 256]\n",
    "# in2 shape [1, entity_vocab, embed_size]\n",
    "# in2 shape [37456, 256]\n",
    "import torch\n",
    "from scipy import spatial\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bffe4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(16, 256, 256)\n",
    "# a = torch.randn(3, 5)\n",
    "b = torch.randn(37456, 256) # different row number, for the fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d129f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# Given that cos_sim(u, v) = dot(u, v) / (norm(u) * norm(v))\n",
    "#                          = dot(u / norm(u), v / norm(v))\n",
    "# We fist normalize the rows, before computing their dot products via transposition:\n",
    "a_norm = a / a.norm(dim=2)[:, :, None]\n",
    "# a_norm = a / a.norm(dim=1)[:, None]\n",
    "b_norm = b / b.norm(dim=1)[:, None]\n",
    "res = torch.matmul(a_norm, b_norm.transpose(0,1))\n",
    "# print(res.shape)\n",
    "print(res)\n",
    "#  0.9978 -0.9986 -0.9985\n",
    "# -0.8629  0.9172  0.9172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f72b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d8420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d6428e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%timeit\n",
    "# # -------\n",
    "# # Let's verify with numpy/scipy if our computations are correct:\n",
    "# a_n = a.numpy()\n",
    "# b_n = b.numpy()\n",
    "# res_n = np.zeros((3,5,50))\n",
    "\n",
    "# for i in range(3):\n",
    "#     for j in range(5):\n",
    "#         for k in range(50):\n",
    "#             # cos_sim(u, v) = 1 - cos_dist(u, v)\n",
    "#             res_n[i, j, k] = 1 - spatial.distance.cosine(a_n[i, j], b_n[k])\n",
    "# print(res_n)\n",
    "# # [[ 0.9978022  -0.99855876 -0.99854881]\n",
    "# #  [-0.86285472  0.91716063  0.9172349 ]]\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "a_n = a.numpy()\n",
    "b_n = b.numpy()\n",
    "\n",
    "res_n = np.zeros((16,256, 37456))\n",
    "for i in range(3):\n",
    "    res_n[i, :, :] = cosine_similarity(a_n[i,:,:], b_n)\n",
    "res_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec68a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_n.shape\n",
    "b_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b5f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix1 = np.array([[[1,1,1],[1,1,1],[1,2,1]],[[1,2,1],[1,1,1],[1,1,1]]])\n",
    "matrix2 = np.array([[1,1,1],[1,2,1], [1,2,1], [1,2,1]])\n",
    "print(matrix1.shape)\n",
    "print(matrix2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb81d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.spatial as sp\n",
    "out = np.zeros((2,3,4))\n",
    "for i in range(2):\n",
    "    out[i, :, :] = cosine_similarity(matrix1[i], matrix2, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b568474",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3e7a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc24481d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "z = torch.randint(-1,3, size=(10,)) \n",
    "print(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef34f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8c6b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1,4,6],[3,5,6]])\n",
    "embedding = torch.randint(10, size=(10, 10))\n",
    "embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0fed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding[x, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f8e470",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x = torch.randint(-1, 5, size  = (10,10))\n",
    "idxs = (x == -1).nonzero()\n",
    "print(x)\n",
    "idxs\n",
    "\n",
    "# new_arr_0 = x[x!=-1]\n",
    "# print(idxs)\n",
    "# new_arr_0\n",
    "\n",
    "# ##### \n",
    "# x = torch.randint(-1, 5, size  = (5,10)).float()\n",
    "# y = torch.randint(-1, 5, size  = (5,10)).float()\n",
    "# import torch.nn.functional as F\n",
    "# F.cosine_similarity(x, y, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c19b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randint(10, size=(10, 10, 10))\n",
    "y[idxs[:, 0], idxs[:, 1]].shape\n",
    "y[idxs[:, 0], idxs[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7abc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "# 2D loss example (used, for example, with image inputs)\n",
    "N, C = 5, 4\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C x height x width\n",
    "data = torch.randn(N, 16, 10, 10)\n",
    "conv = nn.Conv2d(16, C, (3, 3))\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)\n",
    "output = loss(m(conv(data)), target)\n",
    "output.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fe88a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(target.shape)\n",
    "m(conv(data)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eb8723",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944f0f06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# attention_mask torch.Size([27,256])\n",
    "# mention_pred torch.Size([27, 256, 3])\n",
    "# entity_pred torch.Size([27, 256, 256])\n",
    "# bio_tags torch.Size([27, 256])\n",
    "# entity_ids torch.Size([27, 256])\n",
    "\n",
    "# unpadded_men_pred torch.Size([25, 3, 256, 256])\n",
    "# unpadded_bio_tags torch.Size([25, 256, 256])    \n",
    "\n",
    "\n",
    "attention_mask = torch.randint(0,2,size=(3,5))\n",
    "# mention_pred = torch.randn(3,5)\n",
    "losses = torch.randn(3,5)\n",
    "# bio_tags = torch.randn(3,5)\n",
    "print(attention_mask)\n",
    "print(losses)\n",
    "# print(bio_tags)\n",
    "num_unpadded = torch.sum(attention_mask, dim=1)\n",
    "print(\"num_unpadded\",num_unpadded)\n",
    "x = torch.where(attention_mask == 1, losses, torch.tensor(0.).float())\n",
    "print(torch.sum(x, dim=1))\n",
    "torch.sum(torch.sum(x, dim=1)/num_unpadded)/len(x)\n",
    "# print(torch.sum(x, dim=1))\n",
    "# torch.sum(torch.sum(x, dim=1) / num_unpadded)\n",
    "\n",
    "#     unpadded_men_pred = mention_pred[attention_mask,:].permute(0,3,1,2).contiguous()\n",
    "# mention_pred[attention_mask,:]\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d0c4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Construct a random 50x50 RGB image    \n",
    "image = np.random.random((50, 50, 3))\n",
    "\n",
    "# Construct mask according to some condition;\n",
    "# in this case, select all pixels with a red value > 0.3\n",
    "mask = image[..., 0] > 0.3\n",
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b2e475",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b775f4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b770788b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61c4d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(mention_preds, entity_preds, bio_tags, entity_ids, pretrained_entity_embedding):\n",
    "    '''Computes both macro and micro confusion matrices given torch tensor outputs from our model\n",
    "    '''\n",
    "    # mention_pred Size([27, 256, 3])\n",
    "    # entity_pred torch.Size([27, 256, 256])\n",
    "\n",
    "    nb_classes = 2\n",
    "    micro_confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "    # micro_confusion_matrix [[TP, FN], [FP, TN]]\n",
    "#     for t, p in zip(classes.view(-1), preds.view(-1)):\n",
    "#         confusion_matrix[t.long(), p.long()] += 1\n",
    "        \n",
    "    macro_confusion_matrix = []        \n",
    "    \n",
    "    # first we need to find the max of the BIO tagging we have\n",
    "    pred_bio_tags = torch.argmax(mention_preds, dim=2)\n",
    "    # pred_bio_tags torch.Size([28, 256])\n",
    "    \n",
    "    # TODO: Remove after testing\n",
    "    # pred_bio_tags = torch.randint(0,2, size=(28,256))\n",
    "\n",
    "    # create two lists of spans\n",
    "    pred_span_list = get_bi_spans(pred_bio_tags)\n",
    "    target_span_list = get_bi_spans(bio_tags)    \n",
    "    \n",
    "    for batch_idx, (pred_spans, target_spans) in enumerate(zip(pred_span_list, target_span_list)):\n",
    "        tp_matches = np.zeros(target_spans.shape[0], dtype=bool)\n",
    "        \n",
    "        batch_confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
    "\n",
    "        # do the STRONG MATCHING here\n",
    "        for span_idx in range(pred_spans.shape[0]):\n",
    "            if torch.all(torch.tensor(pred_spans[span_idx, :] == target_spans), dim=1).any():\n",
    "                seq_idx = pred_spans[span_idx, 0]\n",
    "                # we use the start span idx to get the entity embedding and compare it to the pretrained embeds\n",
    "#                 print(entity_preds[batch_idx, seq_idx, :].shape)\n",
    "                cos_sim = sim_matrix(entity_preds[batch_idx, seq_idx, :].unsqueeze(0).unsqueeze(0), \n",
    "                                     pretrained_entity_embedding)\n",
    "                # this should return something of size [1, 1, sim_vals]\n",
    "                pred_ent_embed_id = torch.argmax(torch.squeeze(cos_sim))\n",
    "                target_ent_embed_id = entity_ids[batch_idx, seq_idx]\n",
    "                # just to test: \n",
    "                # TODO: REMOVE\n",
    "                if batch_idx == 1 and seq_idx == 0:\n",
    "                    target_ent_embed_id = 2\n",
    "                # TODO: ENDREMOVE\n",
    "                if pred_ent_embed_id == target_ent_embed_id:\n",
    "                    # TP\n",
    "                    micro_confusion_matrix[0,0] += 1\n",
    "                    batch_confusion_matrix[0,0] += 1 \n",
    "                    tp_matches[span_idx] = True\n",
    "                else:\n",
    "                    # FP\n",
    "                    micro_confusion_matrix[1,0] += 1\n",
    "                    batch_confusion_matrix[1,0] += 1\n",
    "            else:\n",
    "                # FP \n",
    "                micro_confusion_matrix[1,0] += 1\n",
    "                batch_confusion_matrix[1,0] += 1\n",
    "\n",
    "        print(tp_matches)        \n",
    "        micro_confusion_matrix[0,1] += np.sum(~tp_matches)\n",
    "        batch_confusion_matrix[0,1] += np.sum(~tp_matches)\n",
    "\n",
    "        macro_confusion_matrix.append(batch_confusion_matrix)\n",
    "\n",
    "        print(micro_confusion_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf92e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "ent_vocab_size = 8\n",
    "import torch.nn.functional as F\n",
    "\n",
    "metric_list = \"\"\n",
    "mention_preds = torch.tensor(np.random.rand(batch_size, 5, 3))\n",
    "# mention_preds = F.softmax(torch.tensor(mention_preds), dim=2)\n",
    "print(mention_preds)\n",
    "entity_preds = torch.tensor(np.random.rand(batch_size, 5, 5))\n",
    "bio_tags = torch.tensor(np.random.randint(0,3, size=(batch_size, 5)))\n",
    "print(bio_tags.shape)\n",
    "print(bio_tags)\n",
    "entity_ids = torch.tensor(np.random.randint(ent_vocab_size, size=(batch_size, 5)))\n",
    "print(entity_ids)\n",
    "pretrained_entity_embedding = torch.tensor(np.random.rand(ent_vocab_size, 5))\n",
    "print(pretrained_entity_embedding.shape)\n",
    "# mention_pred Size([27, 256, 3])\n",
    "# entity_pred torch.Size([27, 256, 256])\n",
    "# bio_tags torch.Size([27, 256])\n",
    "# entity_ids torch.Size([27, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e000f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bio_tags[1,0] = 2\n",
    "# bio_tags[1,4] = 2\n",
    "entity_ids[0,2] = -1\n",
    "entity_ids[1,1] = -1\n",
    "entity_ids[1, 3] = -1\n",
    "entity_ids[1,4] = -1\n",
    "print(bio_tags)\n",
    "entity_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1f3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_metrics(metric_list, mention_preds, entity_preds, bio_tags, entity_ids, pretrained_entity_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5daa33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d018ceaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a631fef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9495c63e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4764ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539e61e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[0, 0, 1, 0, 0],[0, 2, 0, 2, 2]]) ; print(x)\n",
    "y = torch.tensor([[1, 0, 2, 2, 1], [0, 2, 0, 1, 2]]) ; y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a0e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_span = get_bi_spans(x)\n",
    "y_span = get_bi_spans(y)\n",
    "print(x_span)\n",
    "print(y_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_span[0][0,:].shape)\n",
    "print(x_span[0].shape)\n",
    "# (y_span[0][0,:] ==  x_span[0]).shape\n",
    "torch.all(torch.tensor(y_span[0][0,:] == x_span[0]), dim=1).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a816f9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025e8efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(np.array([]).size <= 0):\n",
    "    print(\"X\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "848e3110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.5210)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([-0.3616, -0.6805])\n",
    "torch.sum(x)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03f8093",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
